{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b45c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def perform_statistical_tests(result_df):\n",
    "    # Get unique datasets and methods\n",
    "    datasets = result_df['Dataset'].unique()\n",
    "    methods = result_df['Method'].unique()\n",
    "    \n",
    "    # Remove 'BaLu' from methods list since we're comparing against it\n",
    "    comparison_methods = [method for method in methods if method != 'BaLu']\n",
    "    \n",
    "    # Initialize result DataFrames\n",
    "    pehe_pvalues = pd.DataFrame(index=datasets, columns=comparison_methods)\n",
    "    mae_pvalues = pd.DataFrame(index=datasets, columns=comparison_methods)\n",
    "    \n",
    "    # Perform tests for each dataset and method combination\n",
    "    for dataset in datasets:\n",
    "        # Get BaLu results for this dataset\n",
    "        balu_data = result_df[(result_df['Dataset'] == dataset) & \n",
    "                               (result_df['Method'] == 'BaLu')]\n",
    "        \n",
    "        if len(balu_data) == 0:\n",
    "            print(f\"Warning: No BaLu data found for dataset {dataset}\")\n",
    "            continue\n",
    "            \n",
    "        balu_pehe = balu_data['PEHE'].values\n",
    "        balu_mae = balu_data['MAE'].values\n",
    "        \n",
    "        for method in comparison_methods:\n",
    "            method_data = result_df[(result_df['Dataset'] == dataset) & \n",
    "                                   (result_df['Method'] == method)]\n",
    "            \n",
    "            if len(method_data) == 0:\n",
    "                print(f\"Warning: No {method} data found for dataset {dataset}\")\n",
    "                continue\n",
    "                \n",
    "            method_pehe = method_data['PEHE'].values\n",
    "            method_mae = method_data['MAE'].values\n",
    "            \n",
    "            # Perform Wilcoxon rank-sum test (Mann-Whitney U test)\n",
    "            # This is a non-parametric test that doesn't assume normal distribution\n",
    "            \n",
    "            # Test for PEHE\n",
    "            if len(balu_pehe) > 0 and len(method_pehe) > 0:\n",
    "                _, pehe_pval = stats.mannwhitneyu(balu_pehe, method_pehe, \n",
    "                                                alternative='two-sided')\n",
    "                pehe_pvalues.loc[dataset, method] = pehe_pval\n",
    "            \n",
    "            # Test for MAE\n",
    "            if len(balu_mae) > 0 and len(method_mae) > 0:\n",
    "                _, mae_pval = stats.mannwhitneyu(balu_mae, method_mae, \n",
    "                                               alternative='two-sided')\n",
    "                mae_pvalues.loc[dataset, method] = mae_pval\n",
    "    \n",
    "    return pehe_pvalues, mae_pvalues\n",
    "\n",
    "def add_significance_stars(pvalue_df):\n",
    "    \"\"\"Add significance stars to p-values in scientific notation\"\"\"\n",
    "    starred_df = pvalue_df.copy()\n",
    "    \n",
    "    for col in starred_df.columns:\n",
    "        for idx in starred_df.index:\n",
    "            pval = pd.to_numeric(starred_df.loc[idx, col], errors='coerce')\n",
    "            if pd.notna(pval):\n",
    "                if pval < 0.001:\n",
    "                    starred_df.loc[idx, col] = f\"{pval:.2e}***\"\n",
    "                elif pval < 0.01:\n",
    "                    starred_df.loc[idx, col] = f\"{pval:.2e}**\"\n",
    "                elif pval < 0.05:\n",
    "                    starred_df.loc[idx, col] = f\"{pval:.2e}*\"\n",
    "                else:\n",
    "                    starred_df.loc[idx, col] = f\"{pval:.2e}\"\n",
    "    \n",
    "    return starred_df\n",
    "\n",
    "\n",
    "def format_pvalue_table(pvalue_df, metric_name):\n",
    "    formatted_df = pvalue_df.copy()\n",
    "    \n",
    "    # Convert to numeric and format in scientific notation\n",
    "    for col in formatted_df.columns:\n",
    "        formatted_df[col] = pd.to_numeric(formatted_df[col], errors='coerce')\n",
    "        # Format in scientific notation with 2 decimal places\n",
    "        formatted_df[col] = formatted_df[col].apply(\n",
    "            lambda x: f\"{x:.2e}\" if pd.notna(x) else \"NaN\"\n",
    "        )\n",
    "    \n",
    "    return formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55bd38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fileter_method(must_item: list, method):\n",
    "        method_name = method\n",
    "        for item in must_item:\n",
    "            if item not in method:\n",
    "                return False, None\n",
    "            if \"L=\" in item or 'Balu_plus' in item:\n",
    "                item = item \n",
    "            else:\n",
    "                item = \"_\"+item\n",
    "            method_name = method_name.replace(item, \"\")\n",
    "        return True, method_name\n",
    "\n",
    "def collect_data_p_balu(p: str, datasets=[], result_dir='results', train_test_flag=\"test\"):\n",
    "    if not datasets:\n",
    "        datasets = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = os.path.join(result_dir, dataset)\n",
    "        method_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "        break\n",
    "    \n",
    "    method_dirs = [e for e in method_dirs if 'Balu' in e]\n",
    "\n",
    "    data_rows = []\n",
    "    for method in method_dirs:\n",
    "        flag, method_name = fileter_method(['Balu_plus_imp=', 'L=64-64_K=64', \"gconv=GCN_rconv=GCN\", \"reldrop=0.0_beta=0.0001_gamma=0.0001_eta=0.0001\"], method)\n",
    "        if not flag:\n",
    "            continue\n",
    "        if \"-64\" in method_name:\n",
    "            continue\n",
    "        if \"BaLu\" not in method_name:\n",
    "            continue \n",
    "        \n",
    "        # Collect results for each dataset\n",
    "        for dataset in datasets:\n",
    "            dataset_dir = os.path.join(result_dir, dataset)\n",
    "            \n",
    "            if not os.path.exists(dataset_dir):\n",
    "                print(f\"{dataset_dir} not exits!\")\n",
    "                continue\n",
    "                \n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            if not os.path.exists(method_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "\n",
    "            imputation_dir = method_dir\n",
    "            results_files = [f for f in os.listdir(imputation_dir) \n",
    "                        if f.startswith(f\"p={p}_\") and f.endswith(f\"_{train_test_flag}_results.json\")]\n",
    "            # results_files=results_files[:len(results_files)*1.5//2]\n",
    "\n",
    "            for file in results_files:\n",
    "                file_path = os.path.join(method_dir, file) # imputation, file)\n",
    "                try:\n",
    "                    row = {\n",
    "                        'Method': method,\n",
    "                        # 'imputation': imputation\n",
    "                    }\n",
    "                    row['Dataset'] = dataset\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        results = json.load(f)\n",
    "                        if 'effect_pehe' in results:\n",
    "                            row[f\"PEHE\"] = results['effect_pehe']\n",
    "                            # effect_pehe_values.append(results['effect_pehe'])\n",
    "                        if 'effect_mae' in results:\n",
    "                            row['MAE'] = results['effect_mae']\n",
    "                            # effect_mae_values.append(results['effect_mae'])\n",
    "                except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                    print(f\"Warning: Error reading file {file_path}: {e}\")\n",
    "                data_rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Sort the DataFrame by method and imputation\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['Method']) #, 'imputation'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_data_p_others(p: str, datasets=[], result_dir='results', train_test_flag=\"test\"):\n",
    "    if not datasets:\n",
    "        # If no datasets are specified, get all available datasets from the directory\n",
    "        datasets = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]\n",
    "    \n",
    "    # Initialize an empty list to store data rows\n",
    "    data_rows = []\n",
    "    \n",
    "    # First pass: collect all methods and imputations across all datasets\n",
    "    all_method_imputation_pairs = []\n",
    "    print(datasets)\n",
    "    for dataset in datasets:\n",
    "        dataset_dir = os.path.join(result_dir, dataset)\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            print(f\"Warning: Dataset directory {dataset_dir} not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all method directories for this dataset\n",
    "        method_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "        \n",
    "        method_dirs = [e for e in method_dirs if 'Balu' not in e]\n",
    "\n",
    "        for method in method_dirs:\n",
    "            if method == 'rml':\n",
    "                continue\n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            # print(method_dir)\n",
    "            # print(os.listdir(method_dir))\n",
    "            for imputation in os.listdir(method_dir):\n",
    "                \n",
    "                if p == '0.0' and imputation != 'full':                 # if the missing percentage is 0.0, no evaluation need for other imputation methods\n",
    "                    continue\n",
    "                if p != '0.0' and imputation == 'full':                 # if has missing data, there is no evaluation for complete data (full)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                if imputation not in ['no', 'full']:\n",
    "                    continue\n",
    "                \n",
    "                method_imputation_pair = (method, imputation)\n",
    "                if method_imputation_pair not in all_method_imputation_pairs:\n",
    "                    all_method_imputation_pairs.append(method_imputation_pair)\n",
    "    \n",
    "    # Second pass: collect results for each method/imputation pair across all datasets\n",
    "    for method, imputation in all_method_imputation_pairs:\n",
    "        for dataset in datasets:\n",
    "            \n",
    "            dataset_dir = os.path.join(result_dir, dataset)\n",
    "            if not os.path.exists(dataset_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "                \n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            if not os.path.exists(method_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "            \n",
    "            imputation_dir = os.path.join(method_dir, imputation)\n",
    "            \n",
    "            results_files = [f for f in os.listdir(imputation_dir) \n",
    "                        if f.startswith(f\"p={p}\") and f.endswith(f\"{train_test_flag}_results.json\")]\n",
    "            \n",
    "            \n",
    "            # Collect metrics from all matching files\n",
    "            for file in results_files:\n",
    "                file_path = os.path.join(method_dir, imputation, file)\n",
    "                try:\n",
    "                    row = {\n",
    "                        'Method': method,\n",
    "                        # 'imputation': imputation\n",
    "                    }\n",
    "                    row['Dataset'] = dataset\n",
    "                    filter_th = 40\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        results = json.load(f)\n",
    "                        # if results['effect_pehe'] == np.nan or results['effect_pehe'] > filter_th: continue\n",
    "                        if 'Syn_M=' in dataset and results['effect_pehe'] > 25: continue \n",
    "                        \n",
    "                        # if results['effect_mae'] == np.nan or results['effect_mae'] > filter_th: continue\n",
    "                        if 'effect_pehe' in results:\n",
    "                            row[f\"PEHE\"] = results['effect_pehe']\n",
    "                        if 'effect_mae' in results:\n",
    "                            row['MAE'] = results['effect_mae']\n",
    "                except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                    print(f\"Warning: Error reading file {file_path}: {e}\")\n",
    "            \n",
    "                data_rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Sort the DataFrame by method and imputation\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['Method'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4e4a5",
   "metadata": {},
   "source": [
    "# For Complete Datasets: $p_{miss}=0.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c53ed6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syn_M=None_SimRel=1_Rel=4_MCAR', 'Youtube_M=20_SimRel=1_Rel=4_MCAR', 'BlogCatalog1_M=20_SimRel=0_Rel=1_MCAR', 'Flickr1_M=20_SimRel=0_Rel=1_MCAR']\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "GCN-HSIC & SAGE-HSIC & NetDeconf & SPNet & CausalForest & R-Learner & T-Learner & X-Learner \\\\\n",
      "\\midrule\n",
      "5.76e-06 & 6.61e-08 & 3.71e-04 & 5.29e-02 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "3.14e-02 & 1.59e-04 & 5.98e-01 & 4.57e-01 & 8.57e-02 & 9.86e-02 & 8.32e-03 & 2.38e-02 \\\\\n",
      "6.61e-08 & 6.61e-08 & 1.51e-07 & 6.62e-08 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "6.61e-08 & 6.61e-08 & 6.64e-08 & 6.61e-08 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "GCN-HSIC & SAGE-HSIC & NetDeconf & SPNet & CausalForest & R-Learner & T-Learner & X-Learner \\\\\n",
      "\\midrule\n",
      "2.16e-07 & 4.10e-05 & 3.85e-07 & 9.57e-06 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "1.57e-05 & 1.57e-05 & 3.43e-06 & 2.56e-05 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "6.61e-08 & 3.84e-07 & 1.51e-07 & 6.61e-08 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "6.61e-08 & 1.20e-07 & 6.66e-08 & 3.70e-04 & 6.70e-08 & 1.17e-05 & 6.70e-08 & 6.70e-08 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "P-value tables saved as 'pehe_pvalues.csv' and 'mae_pvalues.csv'\n",
      "\n",
      "PEHE P-values with significance indicators:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN-HSIC</th>\n",
       "      <th>SAGE-HSIC</th>\n",
       "      <th>NetDeconf</th>\n",
       "      <th>SPNet</th>\n",
       "      <th>CausalForest</th>\n",
       "      <th>R-Learner</th>\n",
       "      <th>T-Learner</th>\n",
       "      <th>X-Learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flickr</th>\n",
       "      <td>5.76e-06***</td>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>3.71e-04***</td>\n",
       "      <td>5.29e-02</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlogCatalog</th>\n",
       "      <td>3.14e-02*</td>\n",
       "      <td>1.59e-04***</td>\n",
       "      <td>5.98e-01</td>\n",
       "      <td>4.57e-01</td>\n",
       "      <td>8.57e-02</td>\n",
       "      <td>9.86e-02</td>\n",
       "      <td>8.32e-03**</td>\n",
       "      <td>2.38e-02*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>1.51e-07***</td>\n",
       "      <td>6.62e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Youtube</th>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>6.64e-08***</td>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GCN-HSIC    SAGE-HSIC    NetDeconf        SPNet CausalForest  \\\n",
       "Flickr       5.76e-06***  6.61e-08***  3.71e-04***     5.29e-02  6.70e-08***   \n",
       "BlogCatalog    3.14e-02*  1.59e-04***     5.98e-01     4.57e-01     8.57e-02   \n",
       "Instagram    6.61e-08***  6.61e-08***  1.51e-07***  6.62e-08***  6.70e-08***   \n",
       "Youtube      6.61e-08***  6.61e-08***  6.64e-08***  6.61e-08***  6.70e-08***   \n",
       "\n",
       "               R-Learner    T-Learner    X-Learner  \n",
       "Flickr       1.17e-05***  6.70e-08***  6.70e-08***  \n",
       "BlogCatalog     9.86e-02   8.32e-03**    2.38e-02*  \n",
       "Instagram    1.17e-05***  6.70e-08***  6.70e-08***  \n",
       "Youtube      1.17e-05***  6.70e-08***  6.70e-08***  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE P-values with significance indicators:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN-HSIC</th>\n",
       "      <th>SAGE-HSIC</th>\n",
       "      <th>NetDeconf</th>\n",
       "      <th>SPNet</th>\n",
       "      <th>CausalForest</th>\n",
       "      <th>R-Learner</th>\n",
       "      <th>T-Learner</th>\n",
       "      <th>X-Learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flickr</th>\n",
       "      <td>2.16e-07***</td>\n",
       "      <td>4.10e-05***</td>\n",
       "      <td>3.85e-07***</td>\n",
       "      <td>9.57e-06***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlogCatalog</th>\n",
       "      <td>1.57e-05***</td>\n",
       "      <td>1.57e-05***</td>\n",
       "      <td>3.43e-06***</td>\n",
       "      <td>2.56e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>3.84e-07***</td>\n",
       "      <td>1.51e-07***</td>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Youtube</th>\n",
       "      <td>6.61e-08***</td>\n",
       "      <td>1.20e-07***</td>\n",
       "      <td>6.66e-08***</td>\n",
       "      <td>3.70e-04***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>1.17e-05***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "      <td>6.70e-08***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GCN-HSIC    SAGE-HSIC    NetDeconf        SPNet CausalForest  \\\n",
       "Flickr       2.16e-07***  4.10e-05***  3.85e-07***  9.57e-06***  6.70e-08***   \n",
       "BlogCatalog  1.57e-05***  1.57e-05***  3.43e-06***  2.56e-05***  6.70e-08***   \n",
       "Instagram    6.61e-08***  3.84e-07***  1.51e-07***  6.61e-08***  6.70e-08***   \n",
       "Youtube      6.61e-08***  1.20e-07***  6.66e-08***  3.70e-04***  6.70e-08***   \n",
       "\n",
       "               R-Learner    T-Learner    X-Learner  \n",
       "Flickr       1.17e-05***  6.70e-08***  6.70e-08***  \n",
       "BlogCatalog  1.17e-05***  6.70e-08***  6.70e-08***  \n",
       "Instagram    1.17e-05***  6.70e-08***  6.70e-08***  \n",
       "Youtube      1.17e-05***  6.70e-08***  6.70e-08***  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# from util import *\n",
    "import pandas as pd\n",
    "\n",
    "datasets = ['Syn_M=None_SimRel=1_Rel=4', 'Youtube_M=20_SimRel=1_Rel=4', 'BlogCatalog1_M=20_SimRel=0_Rel=1', 'Flickr1_M=20_SimRel=0_Rel=1']     # , 'BlogCatalog1_M=20_SimRel=1_Rel=4', 'Flickr1_M=20_SimRel=1_Rel=4'\n",
    "datasets = [e+\"_MCAR\" for e in datasets] \n",
    "alian_names = ['Instagram', \"Youtube\", \"BlogCatalog\", \"Flickr\"]\n",
    "dataset_map = {datasets[i]: alian_names[i] for i in range(len(datasets))}\n",
    "\n",
    "p = '0.0'\n",
    "\n",
    "balu_dir = 'results_balu_tuning'\n",
    "df_balu = collect_data_p_balu(result_dir=balu_dir, p=p, datasets=datasets, train_test_flag='test')\n",
    "# df_balu = df_balu.loc[df_balu['Method'] == 'Balu_V5']\n",
    "df_balu['Method'] = df_balu['Method'].apply(lambda x: \"BaLu(-edge)\" if \"IGMC\" in str(x) else \"BaLu\")\n",
    "\n",
    "\n",
    "other_dir = 'results_Q1_MAR_MCAR'\n",
    "df_others = collect_data_p_others(result_dir=other_dir,p=p, datasets=datasets, train_test_flag='test')\n",
    "df_others = df_others.loc[(df_others['Method'] != 'dml')]\n",
    "df_others['Method'] = df_others['Method'].apply(lambda x: \"GCN-HSIC\" if \"GCN\" in str(x) else \"SAGE-HSIC\" if \"GraphSAGE\" in str(x) else x)\n",
    "df_others['Method'] = df_others['Method'].apply(lambda x: \"X-Learner\" if str(x)=='xl' else \"CausalForest\" if str(x)=='cf' else \"T-Learner\" if 'tl' == str(x) else \"R-Learner\" if 'rl' == str(x) else x)\n",
    "\n",
    "####################################################### Merge Dataframes, Oder rows #######################################################\n",
    "result_df = pd.concat([df_balu, df_others], ignore_index=True)\n",
    "result_df['Dataset'] = result_df['Dataset'].apply(lambda x: dataset_map[str(x)])\n",
    "Methods_order = ['BaLu', 'BaLu(-edge)', 'GCN-HSIC', 'SAGE-HSIC', 'NetDeconf', 'SPNet', 'CausalForest', 'R-Learner', 'T-Learner', 'X-Learner']\n",
    "result_df['Method'] = pd.Categorical(result_df['Method'], categories=Methods_order, ordered=True)\n",
    "result_df_ordered = result_df.sort_values('Method')\n",
    "result_df = result_df_ordered.reset_index(drop=True)\n",
    "\n",
    "###################################################################  significance test #######################################################################\n",
    "result_df = result_df.dropna()\n",
    "result_df = result_df.loc[(result_df['Method']!='BaLu(-edge)')]\n",
    "pehe_pvalues, mae_pvalues = perform_statistical_tests(result_df)\n",
    "\n",
    "# Format and display results\n",
    "pehe_formatted = format_pvalue_table(pehe_pvalues, \"PEHE\")\n",
    "mae_formatted = format_pvalue_table(mae_pvalues, \"MAE\")\n",
    "\n",
    "print(pehe_formatted.to_latex(index=False))\n",
    "print(mae_formatted.to_latex(index=False))\n",
    "\n",
    "# Optional: Save results to CSV files\n",
    "# pehe_pvalues.to_csv('pehe_pvalues.csv')\n",
    "# mae_pvalues.to_csv('mae_pvalues.csv')\n",
    "\n",
    "print(\"\\nP-value tables saved as 'pehe_pvalues.csv' and 'mae_pvalues.csv'\")\n",
    "\n",
    "# Optional: Create a combined summary with significance indicators\n",
    "\n",
    "# Create tables with significance stars\n",
    "pehe_with_stars = add_significance_stars(pehe_pvalues)\n",
    "mae_with_stars = add_significance_stars(mae_pvalues)\n",
    "\n",
    "print(\"\\nPEHE P-values with significance indicators:\")\n",
    "display(pehe_with_stars)\n",
    "\n",
    "print(\"\\nMAE P-values with significance indicators:\")\n",
    "display(mae_with_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18605ce4",
   "metadata": {},
   "source": [
    "# For Incomplete Datasets: $p_{miss}=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f80da57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fileter_method(must_item: list, method):\n",
    "        method_name = method\n",
    "        for item in must_item:\n",
    "            if item not in method:\n",
    "                return False, None\n",
    "            if \"L=\" in item or 'Balu_plus' in item:\n",
    "                item = item \n",
    "            else:\n",
    "                item = \"_\"+item\n",
    "            method_name = method_name.replace(item, \"\")\n",
    "        return True, method_name\n",
    "\n",
    "def collect_data_p_balu(p: str, datasets=[], result_dir='results', train_test_flag=\"test\"):\n",
    "    if not datasets:\n",
    "        datasets = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = os.path.join(result_dir, dataset)\n",
    "        method_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "        break\n",
    "    \n",
    "    method_dirs = [e for e in method_dirs if 'Balu' in e]\n",
    "\n",
    "    data_rows = []\n",
    "    for method in method_dirs:\n",
    "        flag, method_name = fileter_method(['Balu_plus_imp=', 'L=64-64_K=64', \"gconv=GCN_rconv=GCN\", \"reldrop=0.0_beta=0.0001_gamma=0.0001_eta=0.0001\"], method)\n",
    "        if not flag:\n",
    "            continue\n",
    "        if \"-64\" in method_name:\n",
    "            continue\n",
    "        if \"BaLu\" not in method_name:\n",
    "            continue \n",
    "        \n",
    "        # Collect results for each dataset\n",
    "        for dataset in datasets:\n",
    "            dataset_dir = os.path.join(result_dir, dataset)\n",
    "            \n",
    "            if not os.path.exists(dataset_dir):\n",
    "                print(f\"{dataset_dir} not exits!\")\n",
    "                continue\n",
    "                \n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            if not os.path.exists(method_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "\n",
    "            imputation_dir = method_dir\n",
    "            results_files = [f for f in os.listdir(imputation_dir) \n",
    "                        if f.startswith(f\"p={p}_\") and f.endswith(f\"_{train_test_flag}_results.json\")]\n",
    "            # results_files=results_files[:len(results_files)*1.5//2]\n",
    "\n",
    "            for file in results_files:\n",
    "                file_path = os.path.join(method_dir, file) # imputation, file)\n",
    "                try:\n",
    "                    row = {\n",
    "                        'Method': method,\n",
    "                        # 'imputation': imputation\n",
    "                    }\n",
    "                    row['Dataset'] = dataset\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        results = json.load(f)\n",
    "                        if 'effect_pehe' in results:\n",
    "                            row[f\"PEHE\"] = results['effect_pehe']\n",
    "                            # effect_pehe_values.append(results['effect_pehe'])\n",
    "                        if 'effect_mae' in results:\n",
    "                            row['MAE'] = results['effect_mae']\n",
    "                            # effect_mae_values.append(results['effect_mae'])\n",
    "                except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                    print(f\"Warning: Error reading file {file_path}: {e}\")\n",
    "                data_rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Sort the DataFrame by method and imputation\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['Method']) #, 'imputation'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_data_p_others(p: str, datasets=[], result_dir='results', train_test_flag=\"test\", best_performance_dict=None):\n",
    "    if not datasets:\n",
    "        # If no datasets are specified, get all available datasets from the directory\n",
    "        datasets = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]\n",
    "    \n",
    "    # Initialize an empty list to store data rows\n",
    "    data_rows = []\n",
    "    \n",
    "    # First pass: collect all methods and imputations across all datasets\n",
    "    all_method_imputation_pairs = []\n",
    "    print(datasets)\n",
    "    for dataset in datasets:\n",
    "        dataset_dir = os.path.join(result_dir, dataset)\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            print(f\"Warning: Dataset directory {dataset_dir} not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all method directories for this dataset\n",
    "        method_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "        \n",
    "        method_dirs = [e for e in method_dirs if 'Balu' not in e]\n",
    "        \n",
    "        for method in method_dirs:\n",
    "            if method == 'rml':\n",
    "                continue\n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            \n",
    "            for imputation in os.listdir(method_dir):\n",
    "                \n",
    "                if p == '0.0' and imputation != 'full':                 # if the missing percentage is 0.0, no evaluation need for other imputation methods\n",
    "                    continue\n",
    "                if p != '0.0' and imputation == 'full':                 # if has missing data, there is no evaluation for complete data (full)\n",
    "                    continue\n",
    "                \n",
    "                if best_performance_dict[dataset][method] != imputation:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                method_imputation_pair = (method, imputation)\n",
    "                if method_imputation_pair not in all_method_imputation_pairs:\n",
    "                    all_method_imputation_pairs.append(method_imputation_pair)\n",
    "    \n",
    "    # Second pass: collect results for each method/imputation pair across all datasets\n",
    "    for method, imputation in all_method_imputation_pairs:\n",
    "        for dataset in datasets:\n",
    "            \n",
    "            dataset_dir = os.path.join(result_dir, dataset)\n",
    "            if not os.path.exists(dataset_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "                \n",
    "            method_dir = os.path.join(dataset_dir, method)\n",
    "            if not os.path.exists(method_dir):\n",
    "                print(f\"{method_dir} not exits!\")\n",
    "                continue\n",
    "            \n",
    "            imputation_dir = os.path.join(method_dir, imputation)\n",
    "            \n",
    "            results_files = [f for f in os.listdir(imputation_dir) \n",
    "                        if f.startswith(f\"p={p}\") and f.endswith(f\"{train_test_flag}_results.json\")]\n",
    "            \n",
    "            \n",
    "            # Collect metrics from all matching files\n",
    "            for file in results_files:\n",
    "                file_path = os.path.join(method_dir, imputation, file)\n",
    "                try:\n",
    "                    row = {\n",
    "                        'Method': method,\n",
    "                        # 'imputation': imputation\n",
    "                    }\n",
    "                    row['Dataset'] = dataset\n",
    "                    filter_th = 40\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        results = json.load(f)\n",
    "                        # if results['effect_pehe'] == np.nan or results['effect_pehe'] > filter_th: continue\n",
    "                        if 'Syn_M=' in dataset and results['effect_pehe'] > 25: continue \n",
    "                        \n",
    "                        # if results['effect_mae'] == np.nan or results['effect_mae'] > filter_th: continue\n",
    "                        if 'effect_pehe' in results:\n",
    "                            row[f\"PEHE\"] = results['effect_pehe']\n",
    "                        if 'effect_mae' in results:\n",
    "                            row['MAE'] = results['effect_mae']\n",
    "                except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                    print(f\"Warning: Error reading file {file_path}: {e}\")\n",
    "            \n",
    "                data_rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Sort the DataFrame by method and imputation\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['Method'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "821e09da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syn_M=None_SimRel=1_Rel=4_MCAR', 'Youtube_M=20_SimRel=1_Rel=4_MCAR', 'BlogCatalog1_M=20_SimRel=0_Rel=1_MCAR', 'Flickr1_M=20_SimRel=0_Rel=1_MCAR']\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "GCN-HSIC & SAGE-HSIC & NetDeconf & SPNet & CausalForest & R-Learner & T-Learner & X-Learner \\\\\n",
      "\\midrule\n",
      "1.18e-04 & 3.63e-10 & 2.25e-02 & 1.26e-02 & 2.49e-07 & 1.05e-07 & 4.41e-10 & 7.22e-10 \\\\\n",
      "2.28e-01 & 4.18e-05 & 4.19e-01 & 7.30e-01 & 1.46e-01 & 1.11e-01 & 7.79e-03 & 1.82e-02 \\\\\n",
      "2.72e-11 & 5.60e-12 & 3.72e-10 & 6.52e-11 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 4.60e-11 \\\\\n",
      "2.72e-11 & 5.60e-12 & 3.72e-10 & 2.71e-11 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 2.93e-11 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "GCN-HSIC & SAGE-HSIC & NetDeconf & SPNet & CausalForest & R-Learner & T-Learner & X-Learner \\\\\n",
      "\\midrule\n",
      "4.96e-07 & 7.19e-09 & 1.61e-02 & 3.21e-05 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 2.72e-11 \\\\\n",
      "7.35e-06 & 1.08e-08 & 4.84e-04 & 1.77e-06 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 2.72e-11 \\\\\n",
      "5.04e-08 & 6.49e-09 & 9.62e-04 & 5.03e-08 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 2.72e-11 \\\\\n",
      "4.03e-05 & 2.94e-08 & 2.26e-06 & 7.77e-08 & 3.01e-09 & 3.01e-09 & 2.72e-11 & 2.72e-11 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "P-value tables saved as 'pehe_pvalues.csv' and 'mae_pvalues.csv'\n",
      "\n",
      "PEHE P-values with significance indicators:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN-HSIC</th>\n",
       "      <th>SAGE-HSIC</th>\n",
       "      <th>NetDeconf</th>\n",
       "      <th>SPNet</th>\n",
       "      <th>CausalForest</th>\n",
       "      <th>R-Learner</th>\n",
       "      <th>T-Learner</th>\n",
       "      <th>X-Learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flickr</th>\n",
       "      <td>1.18e-04***</td>\n",
       "      <td>3.63e-10***</td>\n",
       "      <td>2.25e-02*</td>\n",
       "      <td>1.26e-02*</td>\n",
       "      <td>2.49e-07***</td>\n",
       "      <td>1.05e-07***</td>\n",
       "      <td>4.41e-10***</td>\n",
       "      <td>7.22e-10***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlogCatalog</th>\n",
       "      <td>2.28e-01</td>\n",
       "      <td>4.18e-05***</td>\n",
       "      <td>4.19e-01</td>\n",
       "      <td>7.30e-01</td>\n",
       "      <td>1.46e-01</td>\n",
       "      <td>1.11e-01</td>\n",
       "      <td>7.79e-03**</td>\n",
       "      <td>1.82e-02*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>5.60e-12***</td>\n",
       "      <td>3.72e-10***</td>\n",
       "      <td>6.52e-11***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>4.60e-11***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Youtube</th>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>5.60e-12***</td>\n",
       "      <td>3.72e-10***</td>\n",
       "      <td>2.71e-11***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>2.93e-11***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GCN-HSIC    SAGE-HSIC    NetDeconf        SPNet CausalForest  \\\n",
       "Flickr       1.18e-04***  3.63e-10***    2.25e-02*    1.26e-02*  2.49e-07***   \n",
       "BlogCatalog     2.28e-01  4.18e-05***     4.19e-01     7.30e-01     1.46e-01   \n",
       "Instagram    2.72e-11***  5.60e-12***  3.72e-10***  6.52e-11***  3.01e-09***   \n",
       "Youtube      2.72e-11***  5.60e-12***  3.72e-10***  2.71e-11***  3.01e-09***   \n",
       "\n",
       "               R-Learner    T-Learner    X-Learner  \n",
       "Flickr       1.05e-07***  4.41e-10***  7.22e-10***  \n",
       "BlogCatalog     1.11e-01   7.79e-03**    1.82e-02*  \n",
       "Instagram    3.01e-09***  2.72e-11***  4.60e-11***  \n",
       "Youtube      3.01e-09***  2.72e-11***  2.93e-11***  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE P-values with significance indicators:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCN-HSIC</th>\n",
       "      <th>SAGE-HSIC</th>\n",
       "      <th>NetDeconf</th>\n",
       "      <th>SPNet</th>\n",
       "      <th>CausalForest</th>\n",
       "      <th>R-Learner</th>\n",
       "      <th>T-Learner</th>\n",
       "      <th>X-Learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flickr</th>\n",
       "      <td>4.96e-07***</td>\n",
       "      <td>7.19e-09***</td>\n",
       "      <td>1.61e-02*</td>\n",
       "      <td>3.21e-05***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlogCatalog</th>\n",
       "      <td>7.35e-06***</td>\n",
       "      <td>1.08e-08***</td>\n",
       "      <td>4.84e-04***</td>\n",
       "      <td>1.77e-06***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>5.04e-08***</td>\n",
       "      <td>6.49e-09***</td>\n",
       "      <td>9.62e-04***</td>\n",
       "      <td>5.03e-08***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Youtube</th>\n",
       "      <td>4.03e-05***</td>\n",
       "      <td>2.94e-08***</td>\n",
       "      <td>2.26e-06***</td>\n",
       "      <td>7.77e-08***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>3.01e-09***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "      <td>2.72e-11***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GCN-HSIC    SAGE-HSIC    NetDeconf        SPNet CausalForest  \\\n",
       "Flickr       4.96e-07***  7.19e-09***    1.61e-02*  3.21e-05***  3.01e-09***   \n",
       "BlogCatalog  7.35e-06***  1.08e-08***  4.84e-04***  1.77e-06***  3.01e-09***   \n",
       "Instagram    5.04e-08***  6.49e-09***  9.62e-04***  5.03e-08***  3.01e-09***   \n",
       "Youtube      4.03e-05***  2.94e-08***  2.26e-06***  7.77e-08***  3.01e-09***   \n",
       "\n",
       "               R-Learner    T-Learner    X-Learner  \n",
       "Flickr       3.01e-09***  2.72e-11***  2.72e-11***  \n",
       "BlogCatalog  3.01e-09***  2.72e-11***  2.72e-11***  \n",
       "Instagram    3.01e-09***  2.72e-11***  2.72e-11***  \n",
       "Youtube      3.01e-09***  2.72e-11***  2.72e-11***  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# from util import *\n",
    "import pandas as pd\n",
    "\n",
    "datasets = ['Syn_M=None_SimRel=1_Rel=4', 'Youtube_M=20_SimRel=1_Rel=4', 'BlogCatalog1_M=20_SimRel=0_Rel=1', 'Flickr1_M=20_SimRel=0_Rel=1']     # , 'BlogCatalog1_M=20_SimRel=1_Rel=4', 'Flickr1_M=20_SimRel=1_Rel=4'\n",
    "datasets = [e+\"_MCAR\" for e in datasets] \n",
    "alian_names = ['Instagram', \"Youtube\", \"BlogCatalog\", \"Flickr\"]\n",
    "dataset_map = {datasets[i]: alian_names[i] for i in range(len(datasets))}\n",
    "\n",
    "p = '0.1'\n",
    "\n",
    "balu_dir = 'results_balu_tuning'\n",
    "df_balu = collect_data_p_balu(result_dir=balu_dir, p=p, datasets=datasets, train_test_flag='test')\n",
    "# df_balu = df_balu.loc[df_balu['Method'] == 'Balu_V5']\n",
    "df_balu['Method'] = df_balu['Method'].apply(lambda x: \"BaLu(-edge)\" if \"IGMC\" in str(x) else \"BaLu\")\n",
    "\n",
    "\n",
    "other_dir = 'results_Q1_MAR_MCAR'\n",
    "best_performance_dict ={'Syn_M=None_SimRel=1_Rel=4_MCAR':{'SPNet':'gain', 'GCN_no_drop=0.1_HSIC':'gain', 'dml':'gain', 'cf':'gain', 'tl':'gain', 'GraphSAGE_no_drop=0.1_HSIC':'grape_rel=0.0', 'NetDeconf':'gain', 'rl':'gain', 'xl':'gain'},\n",
    " 'Youtube_M=20_SimRel=1_Rel=4_MCAR':{'SPNet':'gain', 'GCN_no_drop=0.1_HSIC':'gain', 'dml':'gain', 'cf':'gain', 'tl':'gain', 'GraphSAGE_no_drop=0.1_HSIC':'gain', 'NetDeconf':'grape_rel=0.0', 'rl':'gain', 'xl':'gain'},\n",
    " 'BlogCatalog1_M=20_SimRel=0_Rel=1_MCAR':{'SPNet':'mice', 'GCN_no_drop=0.1_HSIC':'mice', 'dml':'gain', 'cf':'grape_rel=0.0', 'tl':'mean', 'GraphSAGE_no_drop=0.1_HSIC':'grape_real', 'NetDeconf':'grape_rel=0.0', 'rl':'missforest', 'xl':'mean'},\n",
    " 'Flickr1_M=20_SimRel=0_Rel=1_MCAR':{'SPNet':'grape_rel=0.0', 'GCN_no_drop=0.1_HSIC':'grape_real', 'dml':'gain', 'cf':'grape_rel=0.0', 'tl':'mice', 'GraphSAGE_no_drop=0.1_HSIC':'missforest', 'NetDeconf':'grape_rel=0.0', 'rl':'mice', 'xl':'knn'}}\n",
    "\n",
    "df_others = collect_data_p_others(result_dir=other_dir,p=p, datasets=datasets, train_test_flag='test', best_performance_dict=best_performance_dict)\n",
    "df_others = df_others.loc[(df_others['Method'] != 'dml')]\n",
    "df_others['Method'] = df_others['Method'].apply(lambda x: \"GCN-HSIC\" if \"GCN\" in str(x) else \"SAGE-HSIC\" if \"GraphSAGE\" in str(x) else x)\n",
    "df_others['Method'] = df_others['Method'].apply(lambda x: \"X-Learner\" if str(x)=='xl' else \"CausalForest\" if str(x)=='cf' else \"T-Learner\" if 'tl' == str(x) else \"R-Learner\" if 'rl' == str(x) else x)\n",
    "\n",
    "####################################################### Merge Dataframes, Oder rows #######################################################\n",
    "result_df = pd.concat([df_balu, df_others], ignore_index=True)\n",
    "result_df['Dataset'] = result_df['Dataset'].apply(lambda x: dataset_map[str(x)])\n",
    "Methods_order = ['BaLu', 'BaLu(-edge)', 'GCN-HSIC', 'SAGE-HSIC', 'NetDeconf', 'SPNet', 'CausalForest', 'R-Learner', 'T-Learner', 'X-Learner']\n",
    "result_df['Method'] = pd.Categorical(result_df['Method'], categories=Methods_order, ordered=True)\n",
    "result_df_ordered = result_df.sort_values('Method')\n",
    "result_df = result_df_ordered.reset_index(drop=True)\n",
    "\n",
    "###################################################################  significance test #######################################################################\n",
    "result_df = result_df.dropna()\n",
    "result_df = result_df.loc[(result_df['Method']!='BaLu(-edge)')]\n",
    "pehe_pvalues, mae_pvalues = perform_statistical_tests(result_df)\n",
    "\n",
    "# Format and display results\n",
    "pehe_formatted = format_pvalue_table(pehe_pvalues, \"PEHE\")\n",
    "mae_formatted = format_pvalue_table(mae_pvalues, \"MAE\")\n",
    "\n",
    "print(pehe_formatted.to_latex(index=False))\n",
    "print(mae_formatted.to_latex(index=False))\n",
    "\n",
    "# Optional: Save results to CSV files\n",
    "# pehe_pvalues.to_csv('pehe_pvalues.csv')\n",
    "# mae_pvalues.to_csv('mae_pvalues.csv')\n",
    "\n",
    "print(\"\\nP-value tables saved as 'pehe_pvalues.csv' and 'mae_pvalues.csv'\")\n",
    "\n",
    "# Optional: Create a combined summary with significance indicators\n",
    "\n",
    "# Create tables with significance stars\n",
    "pehe_with_stars = add_significance_stars(pehe_pvalues)\n",
    "mae_with_stars = add_significance_stars(mae_pvalues)\n",
    "\n",
    "print(\"\\nPEHE P-values with significance indicators:\")\n",
    "display(pehe_with_stars)\n",
    "\n",
    "print(\"\\nMAE P-values with significance indicators:\")\n",
    "display(mae_with_stars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
