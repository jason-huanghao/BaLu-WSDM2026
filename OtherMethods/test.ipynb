{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9435153",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RLearner' from 'econml.dml' (/Users/jason/miniconda3/envs/pytorch/lib/python3.10/site-packages/econml/dml/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01meconml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLearner \n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RLearner' from 'econml.dml' (/Users/jason/miniconda3/envs/pytorch/lib/python3.10/site-packages/econml/dml/__init__.py)"
     ]
    }
   ],
   "source": [
    "from econml.dml import RLearner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "I saw a example : \"\"\"import numpy as np from sklearn.linear_model import LinearRegression from econml.dml._rlearner import RLearner from econml.sklearnextensions.model_selection import SingleModelSelector from sklearn.base import clone class ModelFirst:     def init(self, model):         self._model = clone(model, safe=False)     def fit(self, X, W, Y, sample_weight=None):         self._model.fit(np.hstack([X, W]), Y)         return self     def predict(self, X, W):         return self._model.predict(np.hstack([X, W])) class ModelSelector(SingleModelSelector):     def init(self, model):         self._model = ModelFirst(model)     def train(self, is_selecting, folds, X, W, Y, sample_weight=None):         self._model.fit(X, W, Y, sample_weight=sample_weight)         return self     @property     def best_model(self):         return self._model     @property     def best_score(self):         return 0 class ModelFinal:     def fit(self, X, T, T_res, Y_res, sample_weight=None, freq_weight=None, sample_var=None):         self.model = LinearRegression(fit_intercept=False).fit(X * T_res.reshape(-1, 1),                                                                Y_res)         return self     def predict(self, X):         return self.model.predict(X) class RLearner(_RLearner):     def genmodel_y(self):         return ModelSelector(LinearRegression())     def genmodel_t(self):         return ModelSelector(LinearRegression())     def genrlearner_model_final(self):         return ModelFinal() np.random.seed(123) X = np.random.normal(size=(1000, 3)) y = X[:, 0] + X[:, 1] + np.random.normal(0, 0.01, size=(1000,)) est = RLearner(cv=2, discrete_outcome=False, discrete_treatment=False,                treatment_featurizer=None, categories='auto', random_state=None) est.fit(y, X[:, 0], X=np.ones((X.shape[0], 1)), W=X[:, 1:])\"\"\" from this webpage: https://econml.azurewebsites.net/_autosummary/econml.dml._rlearner.html#econml.dml._rlearner._RLearner. \n",
    "\n",
    "I think it could have similar implementation like other XLearner, TLearner to use the regressor and classifier from the _get_regressor()  and getclassifier() methods as basemodel.  \n",
    "\n",
    "Could you make the RLearner available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c951ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original A (sparse): tensor(indices=tensor([[0, 0, 1, 1, 2, 2, 3, 3, 4, 5],\n",
      "                       [1, 4, 0, 2, 1, 3, 2, 5, 0, 3]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(6, 6), nnz=10, layout=torch.sparse_coo)\n",
      "Original A indices:\n",
      "tensor([[0, 0, 1, 1, 2, 2, 3, 3, 4, 5],\n",
      "        [1, 4, 0, 2, 1, 3, 2, 5, 0, 3]])\n",
      "Original A values:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "Non-null indicator: tensor([ True, False,  True,  True, False,  True])\n",
      "\n",
      "--- Applying the code snippet ---\n",
      "non_null_indices (original indices of non-null rows): tensor([0, 2, 3, 5])\n",
      "\n",
      "mask_rows (source node in non-null set): tensor([ True,  True, False, False,  True,  True,  True,  True, False,  True])\n",
      "mask_cols (target node in non-null set): tensor([False, False,  True,  True, False,  True,  True,  True,  True,  True])\n",
      "\n",
      "valid_edges_mask (both source and target in non-null set): tensor([False, False, False, False, False,  True,  True,  True, False,  True])\n",
      "\n",
      "filtered_A_indices (original indices of kept edges):\n",
      "tensor([[2, 3, 3, 5],\n",
      "        [3, 2, 5, 3]])\n",
      "filtered_A_values (values of kept edges):\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "original_to_new_map (maps original index to new index):\n",
      "tensor([ 0, -1,  1,  2, -1,  3])\n",
      "\n",
      "remap_rows (new indices for source nodes):\n",
      "tensor([1, 2, 2, 3])\n",
      "remap_cols (new indices for target nodes):\n",
      "tensor([2, 1, 3, 2])\n",
      "\n",
      "new_shape: torch.Size([4, 4])\n",
      "\n",
      "--- Result ---\n",
      "Filtered A (sparse): tensor(indices=tensor([[1, 2, 2, 3],\n",
      "                       [2, 1, 3, 2]]),\n",
      "       values=tensor([1., 1., 1., 1.]),\n",
      "       size=(4, 4), nnz=4, layout=torch.sparse_coo)\n",
      "Filtered A indices:\n",
      "tensor([[1, 2, 2, 3],\n",
      "        [2, 1, 3, 2]])\n",
      "Filtered A values:\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "--- Verification ---\n",
      "Original A (dense):\n",
      " [[0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "Expected filtered A (dense, by manual selection):\n",
      "[[0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]]\n",
      "\n",
      "Resulting filtered A (dense):\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "\n",
      "Assertion successful: Filtered A matches expected result!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Helper function (provided in your earlier context)\n",
    "def numpy_adj_to_torch_sparse_tensor(adj_matrix):\n",
    "    \"\"\"\n",
    "    checked!\n",
    "    For data in .mat\n",
    "    \"\"\"\n",
    "    rows, cols = np.nonzero(adj_matrix)\n",
    "    indices = np.stack((rows, cols), axis=0)\n",
    "    indices = torch.from_numpy(indices.astype(np.int64))\n",
    "    num_edges = indices.shape[1]\n",
    "    values = torch.ones(num_edges, dtype=torch.float32) # Assuming all 1s for this example\n",
    "    \n",
    "    shape = torch.Size(adj_matrix.shape)\n",
    "    sparse_tensor = torch.sparse_coo_tensor(indices, values, shape, dtype=torch.float32)\n",
    "    return sparse_tensor\n",
    "\n",
    "# --- Test Data Setup ---\n",
    "\n",
    "# 1. Define an original (dense) adjacency matrix\n",
    "# Let's say we have 6 nodes.\n",
    "# Adjacency matrix:\n",
    "#    0 1 2 3 4 5\n",
    "# 0: 0 1 0 0 1 0\n",
    "# 1: 1 0 1 0 0 0\n",
    "# 2: 0 1 0 1 0 0\n",
    "# 3: 0 0 1 0 0 1\n",
    "# 4: 1 0 0 0 0 0\n",
    "# 5: 0 0 0 1 0 0\n",
    "# Example: Edges are (0,1), (0,4), (1,0), (1,2), (2,1), (2,3), (3,2), (3,5), (4,0), (5,3)\n",
    "original_adj_np = np.array([\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Convert to sparse tensor (this will be A_original)\n",
    "A_original = numpy_adj_to_torch_sparse_tensor(original_adj_np)\n",
    "print(f\"Original A (sparse): {A_original}\")\n",
    "print(f\"Original A indices:\\n{A_original._indices()}\")\n",
    "print(f\"Original A values:\\n{A_original._values()}\\n\")\n",
    "\n",
    "# 2. Simulate which rows are \"non-null\"\n",
    "# Let's say rows 1 and 4 are considered \"null\" and we want to remove them.\n",
    "# So, nodes 0, 2, 3, 5 are non-null.\n",
    "# Original indices: 0, 1, 2, 3, 4, 5\n",
    "# Non-null indicator: T, F, T, T, F, T\n",
    "non_null_indicator = torch.tensor([True, False, True, True, False, True], dtype=torch.bool)\n",
    "print(f\"Non-null indicator: {non_null_indicator}\\n\")\n",
    "\n",
    "# --- Your code snippet to test ---\n",
    "print(\"--- Applying the code snippet ---\")\n",
    "\n",
    "non_null_indices = torch.nonzero(non_null_indicator).squeeze(1)\n",
    "print(f\"non_null_indices (original indices of non-null rows): {non_null_indices}\\n\")\n",
    "\n",
    "A_indices = A_original._indices()\n",
    "A_values = A_original._values()\n",
    "\n",
    "mask_rows = torch.isin(A_indices[0], non_null_indices)\n",
    "mask_cols = torch.isin(A_indices[1], non_null_indices)\n",
    "print(f\"mask_rows (source node in non-null set): {mask_rows}\")\n",
    "print(f\"mask_cols (target node in non-null set): {mask_cols}\\n\")\n",
    "\n",
    "valid_edges_mask = mask_rows & mask_cols\n",
    "print(f\"valid_edges_mask (both source and target in non-null set): {valid_edges_mask}\\n\")\n",
    "\n",
    "filtered_A_indices = A_indices[:, valid_edges_mask]\n",
    "filtered_A_values = A_values[valid_edges_mask]\n",
    "print(f\"filtered_A_indices (original indices of kept edges):\\n{filtered_A_indices}\")\n",
    "print(f\"filtered_A_values (values of kept edges):\\n{filtered_A_values}\\n\")\n",
    "\n",
    "original_to_new_map = -torch.ones(A_original.shape[0], dtype=torch.long)\n",
    "original_to_new_map[non_null_indices] = torch.arange(len(non_null_indices))\n",
    "print(f\"original_to_new_map (maps original index to new index):\\n{original_to_new_map}\\n\")\n",
    "\n",
    "remap_rows = original_to_new_map[filtered_A_indices[0]]\n",
    "remap_cols = original_to_new_map[filtered_A_indices[1]]\n",
    "print(f\"remap_rows (new indices for source nodes):\\n{remap_rows}\")\n",
    "print(f\"remap_cols (new indices for target nodes):\\n{remap_cols}\\n\")\n",
    "\n",
    "new_shape = torch.Size((len(non_null_indices), len(non_null_indices)))\n",
    "print(f\"new_shape: {new_shape}\\n\")\n",
    "\n",
    "A_filtered = torch.sparse_coo_tensor(torch.stack((remap_rows, remap_cols), dim=0), filtered_A_values, new_shape, dtype=torch.float32)\n",
    "\n",
    "print(\"--- Result ---\")\n",
    "print(f\"Filtered A (sparse): {A_filtered}\")\n",
    "print(f\"Filtered A indices:\\n{A_filtered._indices()}\")\n",
    "print(f\"Filtered A values:\\n{A_filtered._values()}\")\n",
    "\n",
    "# Verify by converting to dense and comparing\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(\"Original A (dense):\\n\", A_original.to_dense().numpy())\n",
    "print(\"\\nExpected filtered A (dense, by manual selection):\")\n",
    "# Manual selection: original nodes 0, 2, 3, 5 become new nodes 0, 1, 2, 3\n",
    "expected_dense_filtered_A = np.array([\n",
    "    # Original nodes: 0, 2, 3, 5\n",
    "    # New nodes:      0, 1, 2, 3\n",
    "    [0, 0, 0, 0],  # 0 (was 0) connects to no other non-null\n",
    "    [0, 0, 1, 0],  # 1 (was 2) connects to 2 (was 3)\n",
    "    [0, 1, 0, 1],  # 2 (was 3) connects to 1 (was 2) and 3 (was 5)\n",
    "    [0, 0, 1, 0]   # 3 (was 5) connects to 2 (was 3)\n",
    "])\n",
    "print(expected_dense_filtered_A)\n",
    "\n",
    "print(\"\\nResulting filtered A (dense):\\n\", A_filtered.to_dense().numpy())\n",
    "\n",
    "# Assert for correctness\n",
    "assert np.array_equal(A_filtered.to_dense().numpy(), expected_dense_filtered_A)\n",
    "print(\"\\nAssertion successful: Filtered A matches expected result!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/mats/dataset5/adf/adf/Syn/1.txt\n"
     ]
    }
   ],
   "source": [
    "def create_path(base_path, j):\n",
    "    parts = base_path.split('mats/')\n",
    "    new_path = parts[0] + 'mats/' + f'dataset{j}/' + parts[1]\n",
    "    return new_path\n",
    "\n",
    "# Example usage\n",
    "original_path = 'datasets/mats/adf/adf/Syn/1.txt'\n",
    "j = 5\n",
    "new_path = create_path(original_path, j)\n",
    "print(new_path)  # Output: datasets/mats/dataset5/Syn/1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb32f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape a: (1, 10)\n",
      "Squeezed shape a: (10,)\n",
      "Squeezed a: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Original shape b: (10, 1) 2\n",
      "Squeezed shape b: (10,) 1\n",
      "Squeezed b: [0 1 2 3 4 5 6 7 8 9]\n",
      "squeezed squeezed b: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example 1: array of shape (1, 10)\n",
    "a = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "print(\"Original shape a:\", a.shape)\n",
    "\n",
    "a_squeezed = np.squeeze(a)\n",
    "print(\"Squeezed shape a:\", a_squeezed.shape)\n",
    "print(\"Squeezed a:\", a_squeezed)\n",
    "\n",
    "# Example 2: array of shape (10, 1)\n",
    "b = np.array([[i] for i in range(10)])\n",
    "print(\"\\nOriginal shape b:\", b.shape, len(b.shape))\n",
    "\n",
    "b_squeezed = np.squeeze(b)\n",
    "print(\"Squeezed shape b:\", b_squeezed.shape, len(b_squeezed.shape))\n",
    "print(\"Squeezed b:\", b_squeezed)\n",
    "print(\"squeezed squeezed b:\", np.squeeze(b_squeezed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7debf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "dataset_mat_fn = '/home/jason/coding/NetDeconf_main_hao/datasets/mats/test_imputed=0/Syn/mice/p=0.0_k=2_seed=70.pt.mat'\n",
    "# dataset_mat_fn = '/home/jason/coding/NetDeconf_main_hao/datasets/mats/test_imputed=0/Syn/mice/p=0.0_k=2_seed=70.pt.mat'\n",
    "# dataset_mat_fn = '/home/jason/coding/NetDeconf_main_hao/datasets/mats/test_imputed=0/Syn/mice/p=0.1_k=0_seed=919.pt.mat'\n",
    "\n",
    "data = sio.loadmat(dataset_mat_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a346f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original boolean array: [ True False  True  True False False  True]\n",
      "Data type: bool\n",
      "\n",
      "Using np.where():\n",
      "Integer indices: [0 2 3 6]\n",
      "Data type: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example boolean index array\n",
    "bool_indices = np.array([True, False, True, True, False, False, True])\n",
    "\n",
    "print(\"Original boolean array:\", bool_indices)\n",
    "print(\"Data type:\", bool_indices.dtype)\n",
    "\n",
    "int_indices_where = np.where(bool_indices)[0]\n",
    "\n",
    "print(\"\\nUsing np.where():\")\n",
    "print(\"Integer indices:\", int_indices_where)\n",
    "print(\"Data type:\", int_indices_where.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ef3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/mats/Flickr/no/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/mean/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/knn/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/mice/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/gain/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/no/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/mean/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/knn/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/mice/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/gain/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/no/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/mean/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/knn/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/mice/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/gain/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/no/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/mean/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/knn/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/mice/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/gain/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/no/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/mean/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/knn/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/mice/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/gain/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/no/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/mean/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/knn/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/mice/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/gain/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/no/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/mean/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/knn/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/mice/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/gain/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/no/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/mean/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/knn/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/mice/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/gain/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/no/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/mean/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/knn/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/mice/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/gain/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/no/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/mean/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/knn/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/mice/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/gain/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=5_seed=706\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "########################################## Method Settting  ################################\n",
    "results_dir = 'results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "method = 'NetDeconf'\n",
    "dataset_dir = 'Flickr'\n",
    "missing_p = '0.3'\n",
    "############################################################################################\n",
    "\n",
    "balu_dir = '/mnt/vast-kisski/projects/kisski-tib-activecl/BaLu'\n",
    "source_dir = 'datasets/exps/'\n",
    "target_dir = 'datasets/mats/'\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(balu_dir):\n",
    "    source_dir = os.path.join(balu_dir, source_dir)\n",
    "    target_dir = os.path.join(balu_dir, target_dir)\n",
    "\n",
    "src_dataset = os.path.join(source_dir, dataset_dir)   # source dataset dir\n",
    "tar_dataset = os.path.join(target_dir, dataset_dir)   # target dataset dir\n",
    "\n",
    "dataset_result_dir = os.path.join(results_dir, dataset_dir)\n",
    "os.makedirs(dataset_result_dir, exist_ok=True)\n",
    "\n",
    "method_result_dir = os.path.join(dataset_result_dir, method)\n",
    "os.makedirs(method_result_dir, exist_ok=True)\n",
    "\n",
    "for fn in os.listdir(src_dataset):\n",
    "    if f\"p={missing_p}\" not in fn:\n",
    "        continue\n",
    "    for impute in ['no', 'mean', 'knn', 'mice', 'missforest', 'gain']:\n",
    "        method_impute_dir = os.path.join(method_result_dir, impute)\n",
    "        os.makedirs(method_impute_dir, exist_ok=True)\n",
    "        \n",
    "        tar_dataset_impute = os.path.join(tar_dataset, impute)\n",
    "        data_mat_fn = os.path.join(tar_dataset_impute, fn+\".mat\")\n",
    "        parts = data_mat_fn.split(\"mats/\")[1].split(\"/\")\n",
    "        # dataset = parts[0]; method = parts[1]\n",
    "        identifier = parts[2].split(\".pt\")[0]\n",
    "        one_result_fn = os.path.join(method_impute_dir, identifier)\n",
    "        print(data_mat_fn)\n",
    "        print(one_result_fn)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea39056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/mats/Flickr/no/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/mean/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/knn/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/mice/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/gain/p=0.3_k=9_seed=300.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=9_seed=300\n",
      "datasets/mats/Flickr/no/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/mean/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/knn/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/mice/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/gain/p=0.3_k=6_seed=36.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=6_seed=36\n",
      "datasets/mats/Flickr/no/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/mean/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/knn/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/mice/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/gain/p=0.3_k=0_seed=919.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=0_seed=919\n",
      "datasets/mats/Flickr/no/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/mean/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/knn/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/mice/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/gain/p=0.3_k=8_seed=294.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=8_seed=294\n",
      "datasets/mats/Flickr/no/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/mean/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/knn/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/mice/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/gain/p=0.3_k=1_seed=930.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=1_seed=930\n",
      "datasets/mats/Flickr/no/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/mean/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/knn/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/mice/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/gain/p=0.3_k=2_seed=70.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=2_seed=70\n",
      "datasets/mats/Flickr/no/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/mean/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/knn/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/mice/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/gain/p=0.3_k=7_seed=569.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=7_seed=569\n",
      "datasets/mats/Flickr/no/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/mean/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/knn/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/mice/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/gain/p=0.3_k=4_seed=526.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=4_seed=526\n",
      "datasets/mats/Flickr/no/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/mean/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/knn/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/mice/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/gain/p=0.3_k=3_seed=213.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=3_seed=213\n",
      "datasets/mats/Flickr/no/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/no/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/mean/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/mean/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/knn/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/knn/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/mice/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/mice/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/missforest/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/missforest/p=0.3_k=5_seed=706\n",
      "datasets/mats/Flickr/gain/p=0.3_k=5_seed=706.pt.mat\n",
      "results/Flickr/NetDeconf/gain/p=0.3_k=5_seed=706\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "########################################## Method Settting  ################################\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "method = 'NetDeconf'\n",
    "dataset_dir = 'Flickr'\n",
    "missing_p = '0.3'\n",
    "############################################################################################\n",
    "\n",
    "balu_dir = '/mnt/vast-kisski/projects/kisski-tib-activecl/BaLu'\n",
    "source_dir = 'datasets/exps/'\n",
    "target_dir = 'datasets/mats/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.mkdir(target_dir)\n",
    "\n",
    "if os.path.exists(balu_dir):\n",
    "    source_dir = os.path.join(balu_dir, source_dir)\n",
    "    target_dir = os.path.join(balu_dir, target_dir)\n",
    "\n",
    "src_dataset = os.path.join(source_dir, dataset_dir)   # source dataset dir\n",
    "tar_dataset = os.path.join(target_dir, dataset_dir)   # target dataset dir\n",
    "\n",
    "dataset_result_dir = os.path.join(results_dir, dataset_dir)\n",
    "if not os.path.exists(dataset_result_dir):\n",
    "    os.mkdir(dataset_result_dir)\n",
    "\n",
    "method_result_dir = os.path.join(dataset_result_dir, method)\n",
    "if not os.path.exists(method_result_dir):\n",
    "    os.mkdir(method_result_dir)\n",
    "\n",
    "for fn in os.listdir(src_dataset):\n",
    "    if f\"p={missing_p}\" not in fn:\n",
    "        continue\n",
    "    for impute in ['no', 'mean', 'knn', 'mice', 'missforest', 'gain']:\n",
    "        method_impute_dir = os.path.join(method_result_dir, impute)\n",
    "        if not os.path.exists(method_impute_dir):\n",
    "            os.mkdir(method_impute_dir)\n",
    "        \n",
    "        tar_dataset_impute = os.path.join(tar_dataset, impute)\n",
    "        data_mat_fn = os.path.join(tar_dataset_impute, fn+\".mat\")\n",
    "        parts = data_mat_fn.split(\"mats/\")[1].split(\"/\")\n",
    "        # dataset = parts[0]; method = parts[1]\n",
    "        identifier = parts[2].split(\".pt\")[0]\n",
    "        one_result_fn = os.path.join(method_impute_dir, identifier)\n",
    "        print(data_mat_fn)\n",
    "        print(one_result_fn)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02340d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'b'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
