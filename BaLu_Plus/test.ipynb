{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42ab67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star train\n",
      "train_loss: 1.06930673122406; val_loss: 1.0504820346832275\n",
      "train_loss: 1.045647382736206; val_loss: 1.0294960737228394\n",
      "train_loss: 1.0414314270019531; val_loss: 1.0064090490341187\n",
      "train_loss: 0.9951024055480957; val_loss: 0.9971717000007629\n",
      "train_loss: 0.9847655296325684; val_loss: 0.9799805283546448\n",
      "train_loss: 0.9856551289558411; val_loss: 0.9777274131774902\n",
      "train_loss: 0.9899980425834656; val_loss: 0.9717223048210144\n",
      "train_loss: 0.9629067778587341; val_loss: 0.9561530351638794\n",
      "train_loss: 0.9717110395431519; val_loss: 0.9413155913352966\n",
      "train_loss: 0.9685677886009216; val_loss: 0.9492728114128113\n",
      "train_loss: 0.9609916806221008; val_loss: 0.930394172668457\n",
      "train_loss: 0.9933289885520935; val_loss: 0.944805383682251\n",
      "train_loss: 0.9413076043128967; val_loss: 0.9356354475021362\n",
      "train_loss: 0.9433019757270813; val_loss: 0.9481102228164673\n",
      "train_loss: 0.936301589012146; val_loss: 0.9448279142379761\n",
      "train_loss: 0.9295676350593567; val_loss: 0.9968221783638\n",
      "train_loss: 0.9363701343536377; val_loss: 0.9629036784172058\n",
      "train_loss: 0.9371362328529358; val_loss: 0.9161840081214905\n",
      "train_loss: 0.9320723414421082; val_loss: 0.9492030739784241\n",
      "train_loss: 0.9285153150558472; val_loss: 0.9159293174743652\n",
      "train_loss: 0.9235312938690186; val_loss: 0.9524667859077454\n",
      "train_loss: 0.9184197187423706; val_loss: 0.9292515516281128\n",
      "train_loss: 0.9339470863342285; val_loss: 0.9235436916351318\n",
      "train_loss: 0.9194552302360535; val_loss: 0.9317399859428406\n",
      "train_loss: 0.9505273103713989; val_loss: 0.9382137656211853\n",
      "train_loss: 0.9207670092582703; val_loss: 0.9478943943977356\n",
      "train_loss: 0.9422422051429749; val_loss: 0.9311680197715759\n",
      "train_loss: 0.9343504309654236; val_loss: 0.9142348170280457\n",
      "train_loss: 0.9114380478858948; val_loss: 0.9239634275436401\n",
      "train_loss: 0.9112187027931213; val_loss: 0.9201285243034363\n",
      "train_loss: 0.9251850247383118; val_loss: 0.9100480079650879\n",
      "train_loss: 0.9230064153671265; val_loss: 0.909220814704895\n",
      "train_loss: 0.9223518967628479; val_loss: 0.9027447700500488\n",
      "train_loss: 0.9190202951431274; val_loss: 0.9240904450416565\n",
      "train_loss: 0.9130970239639282; val_loss: 0.9127974510192871\n",
      "train_loss: 0.9202070236206055; val_loss: 0.9023275971412659\n",
      "train_loss: 0.9090823531150818; val_loss: 0.9103963971138\n",
      "train_loss: 0.9262051582336426; val_loss: 0.9091116189956665\n",
      "train_loss: 0.910515308380127; val_loss: 0.9127510786056519\n",
      "train_loss: 0.9134932160377502; val_loss: 0.8996149301528931\n",
      "train_loss: 0.8990490436553955; val_loss: 0.9043858647346497\n",
      "train_loss: 0.8967734575271606; val_loss: 0.905160665512085\n",
      "train_loss: 0.8959464430809021; val_loss: 0.9295145869255066\n",
      "train_loss: 0.9008700847625732; val_loss: 0.9161815047264099\n",
      "train_loss: 0.9001277089118958; val_loss: 0.9075242280960083\n",
      "train_loss: 0.9047707319259644; val_loss: 0.9061557054519653\n",
      "train_loss: 0.9157513380050659; val_loss: 0.9066179990768433\n",
      "train_loss: 0.9136341214179993; val_loss: 0.9095660448074341\n",
      "train_loss: 0.9323060512542725; val_loss: 0.9111383557319641\n",
      "train_loss: 0.8876678943634033; val_loss: 0.9000521302223206\n",
      "train_loss: 0.8952311873435974; val_loss: 0.9136471152305603\n",
      "train_loss: 0.8995693325996399; val_loss: 0.9002307057380676\n",
      "train_loss: 0.9243733882904053; val_loss: 0.9327414631843567\n",
      "train_loss: 0.8943727016448975; val_loss: 0.914112389087677\n",
      "train_loss: 0.905476987361908; val_loss: 0.9099997878074646\n",
      "train_loss: 0.8984659910202026; val_loss: 0.9009378552436829\n",
      "train_loss: 0.9118118286132812; val_loss: 0.9001128673553467\n",
      "train_loss: 0.9121049642562866; val_loss: 0.9072102308273315\n",
      "train_loss: 0.8929629325866699; val_loss: 0.9052545428276062\n",
      "train_loss: 0.8958562612533569; val_loss: 0.8988893628120422\n",
      "train_loss: 0.8913591504096985; val_loss: 0.8968911170959473\n",
      "train_loss: 0.8899824619293213; val_loss: 0.9139865636825562\n",
      "train_loss: 0.8869106769561768; val_loss: 0.8938625454902649\n",
      "train_loss: 0.8870658278465271; val_loss: 0.8970032334327698\n",
      "train_loss: 0.891461193561554; val_loss: 0.9008469581604004\n",
      "train_loss: 0.9082035422325134; val_loss: 0.913249671459198\n",
      "train_loss: 0.8929267525672913; val_loss: 0.8894240260124207\n",
      "train_loss: 0.8780116438865662; val_loss: 0.8912087678909302\n",
      "train_loss: 0.8872536420822144; val_loss: 0.9033756256103516\n",
      "train_loss: 0.8761134743690491; val_loss: 0.9030828475952148\n",
      "train_loss: 0.8728508949279785; val_loss: 0.8999252915382385\n",
      "train_loss: 0.8669039607048035; val_loss: 0.9012140035629272\n",
      "train_loss: 0.8635726571083069; val_loss: 0.8991225957870483\n",
      "train_loss: 0.8800224661827087; val_loss: 0.9085845947265625\n",
      "train_loss: 0.8855456709861755; val_loss: 0.8977101445198059\n",
      "train_loss: 0.867311418056488; val_loss: 0.8996571898460388\n",
      "train_loss: 0.8636565804481506; val_loss: 0.8909798264503479\n",
      "train_loss: 0.8768078684806824; val_loss: 0.8851490616798401\n",
      "train_loss: 0.8654851317405701; val_loss: 0.9275497198104858\n",
      "train_loss: 0.8666297197341919; val_loss: 0.9023579359054565\n",
      "train_loss: 0.8625919818878174; val_loss: 0.90403813123703\n",
      "train_loss: 0.8495538830757141; val_loss: 0.8890265822410583\n",
      "train_loss: 0.8596574664115906; val_loss: 0.8929745554924011\n",
      "train_loss: 0.8624576926231384; val_loss: 0.8910096883773804\n",
      "train_loss: 0.8552613854408264; val_loss: 0.890980064868927\n",
      "train_loss: 0.8548741936683655; val_loss: 0.9004585146903992\n",
      "train_loss: 0.8544949889183044; val_loss: 0.8973257541656494\n",
      "train_loss: 0.8537794351577759; val_loss: 0.9039035439491272\n",
      "train_loss: 0.8568501472473145; val_loss: 0.9051188230514526\n",
      "train_loss: 0.8487041592597961; val_loss: 0.8895143270492554\n",
      "train_loss: 0.8503857254981995; val_loss: 0.8986620903015137\n",
      "train_loss: 0.8503162264823914; val_loss: 0.8933542370796204\n",
      "train_loss: 0.8495665788650513; val_loss: 0.9178545475006104\n",
      "train_loss: 0.8436580300331116; val_loss: 0.898593544960022\n",
      "train_loss: 0.850592315196991; val_loss: 0.8834889531135559\n",
      "train_loss: 0.8416921496391296; val_loss: 0.8797457218170166\n",
      "train_loss: 0.847171425819397; val_loss: 0.8886387944221497\n",
      "train_loss: 0.8380672335624695; val_loss: 0.889326810836792\n",
      "train_loss: 0.8304228186607361; val_loss: 0.8992767333984375\n",
      "train_loss: 0.840526282787323; val_loss: 0.8999595046043396\n",
      "train_loss: 0.8360188603401184; val_loss: 0.8909198045730591\n",
      "train_loss: 0.8318766355514526; val_loss: 0.8855791687965393\n",
      "train_loss: 0.8364037871360779; val_loss: 0.889302134513855\n",
      "train_loss: 0.8404276371002197; val_loss: 0.8988956212997437\n",
      "train_loss: 0.8283004760742188; val_loss: 0.8860054016113281\n",
      "train_loss: 0.8241703510284424; val_loss: 0.8847346901893616\n",
      "train_loss: 0.8319286108016968; val_loss: 0.898048460483551\n",
      "train_loss: 0.8201170563697815; val_loss: 0.8692470788955688\n",
      "train_loss: 0.8255300521850586; val_loss: 0.8779096603393555\n",
      "train_loss: 0.817458987236023; val_loss: 0.8861188888549805\n",
      "train_loss: 0.8203288912773132; val_loss: 0.8822484016418457\n",
      "train_loss: 0.8108816146850586; val_loss: 0.8707994222640991\n",
      "train_loss: 0.823785662651062; val_loss: 0.8709608912467957\n",
      "train_loss: 0.8132998943328857; val_loss: 0.8716758489608765\n",
      "train_loss: 0.8042892813682556; val_loss: 0.874621570110321\n",
      "train_loss: 0.8070325255393982; val_loss: 0.8722665905952454\n",
      "train_loss: 0.8033730983734131; val_loss: 0.8943840265274048\n",
      "train_loss: 0.8077474236488342; val_loss: 0.8642529249191284\n",
      "train_loss: 0.8048136234283447; val_loss: 0.8650640249252319\n",
      "train_loss: 0.7994816899299622; val_loss: 0.8789892196655273\n",
      "train_loss: 0.7959861159324646; val_loss: 0.8504159450531006\n",
      "train_loss: 0.7928400635719299; val_loss: 0.8563479781150818\n",
      "train_loss: 0.789401650428772; val_loss: 0.8558838963508606\n",
      "train_loss: 0.7888714075088501; val_loss: 0.8462255597114563\n",
      "train_loss: 0.7837949991226196; val_loss: 0.8461648225784302\n",
      "train_loss: 0.7835151553153992; val_loss: 0.8505411744117737\n",
      "train_loss: 0.7888554334640503; val_loss: 0.8519856929779053\n",
      "train_loss: 0.7808821201324463; val_loss: 0.861496090888977\n",
      "train_loss: 0.7785924077033997; val_loss: 0.8565552234649658\n",
      "train_loss: 0.7720405459403992; val_loss: 0.8712750673294067\n",
      "train_loss: 0.7684390544891357; val_loss: 0.8564376831054688\n",
      "train_loss: 0.7655224204063416; val_loss: 0.8499611616134644\n",
      "train_loss: 0.7582467198371887; val_loss: 0.8494797945022583\n",
      "train_loss: 0.7588880062103271; val_loss: 0.8271375894546509\n",
      "train_loss: 0.765555739402771; val_loss: 0.8373528718948364\n",
      "train_loss: 0.7569860816001892; val_loss: 0.8614572882652283\n",
      "train_loss: 0.7517547607421875; val_loss: 0.8424314856529236\n",
      "train_loss: 0.7499577403068542; val_loss: 0.8266186118125916\n",
      "train_loss: 0.7413088083267212; val_loss: 0.838539719581604\n",
      "train_loss: 0.7490715980529785; val_loss: 0.8497165441513062\n",
      "train_loss: 0.7405761480331421; val_loss: 0.8425039052963257\n",
      "train_loss: 0.7354223728179932; val_loss: 0.8245392441749573\n",
      "train_loss: 0.7362486124038696; val_loss: 0.828956663608551\n",
      "train_loss: 0.7408020496368408; val_loss: 0.8342145085334778\n",
      "train_loss: 0.7262284755706787; val_loss: 0.8231670260429382\n",
      "train_loss: 0.7358301877975464; val_loss: 0.8291444778442383\n",
      "train_loss: 0.7214791178703308; val_loss: 0.8397160768508911\n",
      "train_loss: 0.7194655537605286; val_loss: 0.8034707903862\n",
      "train_loss: 0.7291153073310852; val_loss: 0.8167324662208557\n",
      "train_loss: 0.7211617827415466; val_loss: 0.8313781023025513\n",
      "train_loss: 0.7136662006378174; val_loss: 0.8235756158828735\n",
      "train_loss: 0.7114471197128296; val_loss: 0.816787600517273\n",
      "train_loss: 0.7114897966384888; val_loss: 0.816728949546814\n",
      "train_loss: 0.7026749849319458; val_loss: 0.8093165755271912\n",
      "train_loss: 0.7009292840957642; val_loss: 0.8012419939041138\n",
      "train_loss: 0.7060461640357971; val_loss: 0.8216230869293213\n",
      "train_loss: 0.7029926180839539; val_loss: 0.8000800609588623\n",
      "train_loss: 0.6994832754135132; val_loss: 0.8088365197181702\n",
      "train_loss: 0.7008035182952881; val_loss: 0.7955377101898193\n",
      "train_loss: 0.7014312148094177; val_loss: 0.798283576965332\n",
      "train_loss: 0.6869240999221802; val_loss: 0.8047155737876892\n",
      "train_loss: 0.6859488487243652; val_loss: 0.7705751657485962\n",
      "train_loss: 0.6852673292160034; val_loss: 0.8331676125526428\n",
      "train_loss: 0.6766782999038696; val_loss: 0.7905648946762085\n",
      "train_loss: 0.6782856583595276; val_loss: 0.7945706844329834\n",
      "train_loss: 0.6870966553688049; val_loss: 0.7863479256629944\n",
      "train_loss: 0.6774681210517883; val_loss: 0.7986804246902466\n",
      "train_loss: 0.6655778288841248; val_loss: 0.7742282152175903\n",
      "train_loss: 0.6723235845565796; val_loss: 0.8041700720787048\n",
      "train_loss: 0.6707346439361572; val_loss: 0.7798096537590027\n",
      "train_loss: 0.6723547577857971; val_loss: 0.8000844120979309\n",
      "train_loss: 0.666068434715271; val_loss: 0.7650334239006042\n",
      "train_loss: 0.6713972091674805; val_loss: 0.7827481627464294\n",
      "train_loss: 0.6577280759811401; val_loss: 0.7933895587921143\n",
      "train_loss: 0.6673996448516846; val_loss: 0.8053838014602661\n",
      "train_loss: 0.66287761926651; val_loss: 0.7621024250984192\n",
      "train_loss: 0.6635456681251526; val_loss: 0.7928755879402161\n",
      "train_loss: 0.64824378490448; val_loss: 0.7842608690261841\n",
      "train_loss: 0.6618706583976746; val_loss: 0.7736954689025879\n",
      "train_loss: 0.6490616202354431; val_loss: 0.7837687730789185\n",
      "train_loss: 0.6544979214668274; val_loss: 0.7989882230758667\n",
      "train_loss: 0.6492182016372681; val_loss: 0.7876709699630737\n",
      "train_loss: 0.6417850255966187; val_loss: 0.7737957239151001\n",
      "train_loss: 0.6514317989349365; val_loss: 0.7539466619491577\n",
      "train_loss: 0.648368239402771; val_loss: 0.7628293633460999\n",
      "train_loss: 0.637744128704071; val_loss: 0.761782705783844\n",
      "train_loss: 0.6391481757164001; val_loss: 0.8037216663360596\n",
      "train_loss: 0.6493091583251953; val_loss: 0.7636920809745789\n",
      "train_loss: 0.6314387321472168; val_loss: 0.7501595616340637\n",
      "train_loss: 0.6443544626235962; val_loss: 0.7540121674537659\n",
      "train_loss: 0.6365896463394165; val_loss: 0.7764009237289429\n",
      "train_loss: 0.6279691457748413; val_loss: 0.7932445406913757\n",
      "train_loss: 0.633461594581604; val_loss: 0.7666559815406799\n",
      "train_loss: 0.6371529698371887; val_loss: 0.7867969274520874\n",
      "train_loss: 0.6382771730422974; val_loss: 0.7839617729187012\n",
      "train_loss: 0.6041979193687439; val_loss: 0.775058388710022\n",
      "train_loss: 0.6125762462615967; val_loss: 0.777895987033844\n",
      "train_loss: 0.6256899237632751; val_loss: 0.7650392055511475\n",
      "train_loss: 0.6301668286323547; val_loss: 0.7680553197860718\n",
      "train_loss: 0.6261851191520691; val_loss: 0.786672830581665\n",
      "train_loss: 0.6213718056678772; val_loss: 0.7575259804725647\n",
      "train_loss: 0.6226595044136047; val_loss: 0.7529416084289551\n",
      "train_loss: 0.6258386969566345; val_loss: 0.7540894150733948\n",
      "train_loss: 0.6107805967330933; val_loss: 0.7721600532531738\n",
      "train_loss: 0.6210759878158569; val_loss: 0.7781813144683838\n",
      "train_loss: 0.6029291749000549; val_loss: 0.7279865741729736\n",
      "train_loss: 0.604069709777832; val_loss: 0.7500914335250854\n",
      "train_loss: 0.5995860695838928; val_loss: 0.7346990704536438\n",
      "train_loss: 0.5995086431503296; val_loss: 0.7403874397277832\n",
      "train_loss: 0.6043305397033691; val_loss: 0.7532519102096558\n",
      "train_loss: 0.6190056204795837; val_loss: 0.7599998116493225\n",
      "train_loss: 0.6076430678367615; val_loss: 0.7326650023460388\n",
      "train_loss: 0.5980516672134399; val_loss: 0.7273547649383545\n",
      "train_loss: 0.6037575006484985; val_loss: 0.7483665347099304\n",
      "train_loss: 0.5914657115936279; val_loss: 0.7343043684959412\n",
      "train_loss: 0.6079495549201965; val_loss: 0.7778003215789795\n",
      "train_loss: 0.5983665585517883; val_loss: 0.7365795373916626\n",
      "train_loss: 0.5846623182296753; val_loss: 0.7454858422279358\n",
      "train_loss: 0.5896131992340088; val_loss: 0.747379720211029\n",
      "train_loss: 0.5872806310653687; val_loss: 0.7469916939735413\n",
      "train_loss: 0.5793837904930115; val_loss: 0.7284780144691467\n",
      "train_loss: 0.5806431770324707; val_loss: 0.7425400018692017\n",
      "train_loss: 0.5863330364227295; val_loss: 0.73832106590271\n",
      "train_loss: 0.5757738351821899; val_loss: 0.7525138258934021\n",
      "train_loss: 0.5792431831359863; val_loss: 0.7418166995048523\n",
      "train_loss: 0.5738537311553955; val_loss: 0.7567354440689087\n",
      "train_loss: 0.5756192803382874; val_loss: 0.730739414691925\n",
      "train_loss: 0.5761268138885498; val_loss: 0.7358619570732117\n",
      "train_loss: 0.5653037428855896; val_loss: 0.7303178310394287\n",
      "train_loss: 0.5773294568061829; val_loss: 0.7611871957778931\n",
      "train_loss: 0.5694884657859802; val_loss: 0.7671648263931274\n",
      "train_loss: 0.5668118596076965; val_loss: 0.7553819417953491\n",
      "train_loss: 0.5572496652603149; val_loss: 0.7635889649391174\n",
      "train_loss: 0.5753575563430786; val_loss: 0.7337393164634705\n",
      "train_loss: 0.5591084361076355; val_loss: 0.7225677967071533\n",
      "train_loss: 0.5687220096588135; val_loss: 0.7594927549362183\n",
      "train_loss: 0.5659574270248413; val_loss: 0.736251711845398\n",
      "train_loss: 0.5790866613388062; val_loss: 0.7473946213722229\n",
      "train_loss: 0.5703986883163452; val_loss: 0.7540920376777649\n",
      "train_loss: 0.5610138773918152; val_loss: 0.7309839129447937\n",
      "train_loss: 0.5424138307571411; val_loss: 0.742664098739624\n",
      "train_loss: 0.5472559928894043; val_loss: 0.7364903092384338\n",
      "train_loss: 0.5543395280838013; val_loss: 0.7267974019050598\n",
      "train_loss: 0.5396660566329956; val_loss: 0.7235068082809448\n",
      "train_loss: 0.5577908754348755; val_loss: 0.7597971558570862\n",
      "train_loss: 0.5578429698944092; val_loss: 0.7628017067909241\n",
      "train_loss: 0.5445137619972229; val_loss: 0.7246904373168945\n",
      "train_loss: 0.5575652718544006; val_loss: 0.7442152500152588\n",
      "train_loss: 0.554274320602417; val_loss: 0.7612045407295227\n",
      "train_loss: 0.5406163334846497; val_loss: 0.7216542959213257\n",
      "train_loss: 0.560930609703064; val_loss: 0.7616228461265564\n",
      "train_loss: 0.5349182486534119; val_loss: 0.7041187882423401\n",
      "train_loss: 0.5490157008171082; val_loss: 0.732252836227417\n",
      "train_loss: 0.5471489429473877; val_loss: 0.7643759846687317\n",
      "train_loss: 0.5373861193656921; val_loss: 0.748195469379425\n",
      "train_loss: 0.5400548577308655; val_loss: 0.7186790108680725\n",
      "train_loss: 0.5432401299476624; val_loss: 0.7005733251571655\n",
      "train_loss: 0.534371018409729; val_loss: 0.7102017402648926\n",
      "train_loss: 0.5354903936386108; val_loss: 0.7421332001686096\n",
      "train_loss: 0.5390957593917847; val_loss: 0.7505235075950623\n",
      "train_loss: 0.5172668695449829; val_loss: 0.7307437062263489\n",
      "train_loss: 0.5388044714927673; val_loss: 0.7036733627319336\n",
      "train_loss: 0.533626914024353; val_loss: 0.7205307483673096\n",
      "train_loss: 0.5259497761726379; val_loss: 0.7343284487724304\n",
      "train_loss: 0.5147976875305176; val_loss: 0.7519964575767517\n",
      "train_loss: 0.5242077708244324; val_loss: 0.7292713522911072\n",
      "train_loss: 0.5301513671875; val_loss: 0.7262918949127197\n",
      "train_loss: 0.5271564722061157; val_loss: 0.7229286432266235\n",
      "train_loss: 0.5178427696228027; val_loss: 0.7212891578674316\n",
      "train_loss: 0.5281168818473816; val_loss: 0.7525765895843506\n",
      "train_loss: 0.5012990236282349; val_loss: 0.7606129050254822\n",
      "train_loss: 0.511691689491272; val_loss: 0.7341546416282654\n",
      "train_loss: 0.5113630294799805; val_loss: 0.725379467010498\n",
      "train_loss: 0.4989657402038574; val_loss: 0.7094123363494873\n",
      "train_loss: 0.5097154378890991; val_loss: 0.6952788233757019\n",
      "train_loss: 0.4968602657318115; val_loss: 0.7062563896179199\n",
      "train_loss: 0.5067282319068909; val_loss: 0.7287411689758301\n",
      "train_loss: 0.5013024806976318; val_loss: 0.7253987193107605\n",
      "train_loss: 0.5098316073417664; val_loss: 0.7262002825737\n",
      "train_loss: 0.5029908418655396; val_loss: 0.684105634689331\n",
      "train_loss: 0.5003555417060852; val_loss: 0.7180866003036499\n",
      "train_loss: 0.512292742729187; val_loss: 0.71816086769104\n",
      "train_loss: 0.5004207491874695; val_loss: 0.7296125292778015\n",
      "train_loss: 0.49229687452316284; val_loss: 0.7140035033226013\n",
      "train_loss: 0.4962104558944702; val_loss: 0.7190078496932983\n",
      "train_loss: 0.484437495470047; val_loss: 0.7269437909126282\n",
      "train_loss: 0.4867434501647949; val_loss: 0.7127975225448608\n",
      "train_loss: 0.5055227279663086; val_loss: 0.7219133973121643\n",
      "train_loss: 0.48083898425102234; val_loss: 0.6942521929740906\n",
      "train_loss: 0.49831777811050415; val_loss: 0.6998300552368164\n",
      "train_loss: 0.4894176423549652; val_loss: 0.6972607374191284\n",
      "train_loss: 0.48993197083473206; val_loss: 0.7236518263816833\n",
      "train_loss: 0.46795007586479187; val_loss: 0.7684042453765869\n",
      "train_loss: 0.4928150773048401; val_loss: 0.6897613406181335\n",
      "train_loss: 0.4865158498287201; val_loss: 0.7205578684806824\n",
      "train_loss: 0.4836133122444153; val_loss: 0.6982219219207764\n",
      "train_loss: 0.4707399606704712; val_loss: 0.7242195010185242\n",
      "train_loss: 0.48244377970695496; val_loss: 0.7440141439437866\n",
      "train_loss: 0.5083256363868713; val_loss: 0.7010774612426758\n",
      "train_loss: 0.4764685034751892; val_loss: 0.7114126086235046\n",
      "train_loss: 0.48457109928131104; val_loss: 0.7292208075523376\n",
      "train_loss: 0.4699108302593231; val_loss: 0.7075188755989075\n",
      "train_loss: 0.4622543454170227; val_loss: 0.7320531010627747\n",
      "train_loss: 0.46433335542678833; val_loss: 0.7264407277107239\n",
      "train_loss: 0.47259658575057983; val_loss: 0.7238301634788513\n",
      "Early stopping!\n",
      "finished train\n",
      "Evaluation Results: {'effect_pehe': 6.1644673347473145, 'effect_mae': 0.9073848724365234}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from src.train_module import ModelTrainer\n",
    "from src.balu import BaLu\n",
    "from argparse import Namespace\n",
    "from src.train_utils import filter_dataset\n",
    "\n",
    "full_data_path = '/Users/jason/Documents/Coding Projects/2025_Claude/NetDeconf_main_hao/datasets/exps/BlogCatalog/p=0.0_k=9_seed=194.pt'\n",
    "# full_data_path = 'datasets/exps/Syn/p=0.0_k=0_seed=919.pt'\n",
    "\n",
    "data = torch.load(full_data_path, weights_only=False)\n",
    "\n",
    "seed = 460\n",
    "params = {\n",
    "    'dataset': 'Syn',         # REQUIRED: replace with actual dataset name\n",
    "    'missing_p': 0.1,                        # REQUIRED: replace with actual missing percentage\n",
    "\n",
    "    'model_name': 'BaLu_Ultra',\n",
    "\n",
    "    'gconv': 'GraphSAGE',\n",
    "    'rconv': 'RGCN',\n",
    "\n",
    "    'imputer': 'BaLu_IGMC',\n",
    "    'imputer_node_dims': [64, 64, 32],\n",
    "\n",
    "    'edge_dim': 16,\n",
    "    'dropout': 0.1,\n",
    "    'rel_dropout': 0.2,\n",
    "\n",
    "    'interference': 'GNN',\n",
    "    'interference_node_dims': [64, 32],\n",
    "\n",
    "    'outcome_rep': 'h_r+X*+h_t',\n",
    "\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'n_epochs': 2000,\n",
    "\n",
    "    'norm_y': True,\n",
    "\n",
    "    'alpha': 1.0,\n",
    "    'beta': 1e-4,\n",
    "    'gamma': 1e-4,\n",
    "    'eta': 1e-4,\n",
    "\n",
    "    'early_stop': True,\n",
    "    'patience': 25,\n",
    "}\n",
    "# params = Namespace(**params)\n",
    "\n",
    "trainer = ModelTrainer(BaLu, params, seed=seed)\n",
    "\n",
    "train_data = filter_dataset(data, data.val_mask | data.train_mask)\n",
    "print(\"star train\")\n",
    "best_model = trainer.train(train_data)# data)\n",
    "print(\"finished train\")\n",
    "results = trainer.evaluate(data, unit_indexes=data.test_mask)\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782a616",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e4353",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5196] at index 0 does not match the shape of the indexed tensor [5216, 20] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 200\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# full_data_path = 'datasets/exps/Syn/p=0.0_k=0_seed=919.pt'\u001b[39;00m\n\u001b[1;32m    199\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(full_data_path, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 200\u001b[0m \u001b[43mcreate_train_data_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 182\u001b[0m, in \u001b[0;36mcreate_train_data_only\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_train_data_only\u001b[39m(data: Data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Data:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Create dataset with only training nodes.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfilter_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mfilter_dataset\u001b[0;34m(data, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 1. Filter node-level tensors\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     train_data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mtreatment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     train_data\u001b[38;5;241m.\u001b[39mtreatment \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtreatment[mask]\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [5196] at index 0 does not match the shape of the indexed tensor [5216, 20] at index 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def filter_and_remap_edges(edge_index, node_mask):\n",
    "    \"\"\"\n",
    "    Filter edges and remap node indices to consecutive integers.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: torch.Tensor of shape [2, E] \n",
    "        node_mask: torch.Tensor of shape [N,] with boolean values indicating valid nodes\n",
    "    \n",
    "    Returns:\n",
    "        new_edge_index: torch.Tensor of shape [2, E'] with filtered and remapped edges\n",
    "    \"\"\"\n",
    "    # Create mapping using cumsum (efficient)\n",
    "    mapping = torch.cumsum(node_mask, dim=0) - 1\n",
    "    mapping[~node_mask] = -1  # Mark invalid nodes\n",
    "    \n",
    "    # Filter and remap edges\n",
    "    edge_mask = node_mask[edge_index[0]] & node_mask[edge_index[1]]\n",
    "    filtered_edges = edge_index[:, edge_mask]\n",
    "    \n",
    "    # Apply mapping directly\n",
    "    new_edge_index = mapping[filtered_edges]\n",
    "    \n",
    "    return new_edge_index\n",
    "\n",
    "\n",
    "def filter_dataset(data: Data, mask: torch.Tensor) -> Data:\n",
    "    \"\"\"\n",
    "    Filter dataset to keep only nodes where mask[i] = True.\n",
    "    \n",
    "    Args:\n",
    "        data: PyTorch Geometric Data object\n",
    "        mask: Boolean tensor of shape [N,] where N is the number of nodes\n",
    "    \n",
    "    Returns:\n",
    "        train_data: New Data object with filtered nodes and remapped structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure mask is boolean and on CPU for numpy operations\n",
    "    mask = mask.bool()\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    \n",
    "    # Create new data object\n",
    "    train_data = Data()\n",
    "    \n",
    "    n_units, n_attrs = data.n_units, data.n_attrs\n",
    "\n",
    "    # 1. Filter node-level tensors\n",
    "    if hasattr(data, 'x') and data.x is not None:\n",
    "        train_data.x = torch.cat([data.x[:n_units][mask] , data.x[n_units:]], dim=0)\n",
    "    \n",
    "    if hasattr(data, 'treatment') and data.treatment is not None:\n",
    "        train_data.treatment = data.treatment[mask]\n",
    "    \n",
    "    if hasattr(data, 'outcome') and data.outcome is not None:\n",
    "        train_data.outcome = data.outcome[mask]\n",
    "    \n",
    "    if hasattr(data, 'true_effect') and data.true_effect is not None:\n",
    "        train_data.true_effect = data.true_effect[mask]\n",
    "    \n",
    "    if hasattr(data, 'is_unit') and data.is_unit is not None:\n",
    "        train_data.is_unit = torch.cat([data.is_unit[:n_units][mask] , data.is_unit[n_units:]], dim=0) #data.is_unit[mask]\n",
    "    \n",
    "    # 2. Filter mask tensors\n",
    "    attrs_mask = mask.repeat_interleave(n_attrs)\n",
    "    \n",
    "    if hasattr(data, 'observed_mask') and data.observed_mask is not None:\n",
    "        train_data.observed_mask = data.observed_mask[attrs_mask]\n",
    "    \n",
    "    if hasattr(data, 'treatment_mask') and data.treatment_mask is not None:\n",
    "        train_data.treatment_mask = data.treatment_mask[mask]\n",
    "    \n",
    "    if hasattr(data, 'outcome_mask') and data.outcome_mask is not None:\n",
    "        train_data.outcome_mask = data.outcome_mask[mask]\n",
    "    \n",
    "    # 3. Filter split masks\n",
    "    if hasattr(data, 'train_mask') and data.train_mask is not None:\n",
    "        train_data.train_mask = data.train_mask[mask]\n",
    "    \n",
    "    if hasattr(data, 'val_mask') and data.val_mask is not None:\n",
    "        train_data.val_mask = data.val_mask[mask]\n",
    "    \n",
    "    if hasattr(data, 'test_mask') and data.test_mask is not None:\n",
    "        train_data.test_mask = data.test_mask[mask]\n",
    "    \n",
    "    # 4. Filter and remap edge structures\n",
    "    if hasattr(data, 'edge_index') and data.edge_index is not None:\n",
    "        train_data.edge_index = filter_and_remap_edges(data.edge_index, mask)\n",
    "        \n",
    "        # Filter edge attributes if they exist\n",
    "        if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "            edge_mask = mask[data.edge_index[0]] & mask[data.edge_index[1]]\n",
    "            train_data.edge_attr = data.edge_attr[edge_mask]\n",
    "    \n",
    "    # 5. Filter relational edges\n",
    "    if hasattr(data, 'rel_edge_index') and data.rel_edge_index is not None:\n",
    "        train_data.rel_edge_index = filter_and_remap_edges(data.rel_edge_index, mask)\n",
    "        \n",
    "        # Filter relational edge types\n",
    "        if hasattr(data, 'rel_edge_type') and data.rel_edge_type is not None:\n",
    "            rel_edge_mask = mask[data.rel_edge_index[0]] & mask[data.rel_edge_index[1]]\n",
    "            train_data.rel_edge_type = data.rel_edge_type[rel_edge_mask]\n",
    "    \n",
    "    # 6. Update metadata\n",
    "    train_data.n_units = mask.sum().item()\n",
    "    \n",
    "    # Copy unchanged metadata\n",
    "    for attr in ['n_attrs', 'n_rel_types', 'node_feature_dim', 'edge_attr_dim']:\n",
    "        if hasattr(data, attr):\n",
    "            setattr(train_data, attr, getattr(data, attr))\n",
    "    \n",
    "    # 7. Filter adjacency matrix (for network baselines)\n",
    "    if hasattr(data, 'A') and data.A is not None:\n",
    "        # Handle sparse tensor\n",
    "        if hasattr(data.A, 'to_dense'):\n",
    "            adj_dense = data.A.to_dense().cpu().numpy()\n",
    "        else:\n",
    "            adj_dense = data.A.cpu().numpy()\n",
    "        \n",
    "        # Filter adjacency matrix\n",
    "        node_indices = np.where(mask_np)[0]\n",
    "        filtered_adj = adj_dense[np.ix_(node_indices, node_indices)]\n",
    "        \n",
    "        # Convert back to same format as original\n",
    "        if hasattr(data.A, 'to_dense'):\n",
    "            # Was sparse, convert back to sparse\n",
    "            train_data.A = torch.sparse_coo_tensor(\n",
    "                indices=torch.nonzero(torch.tensor(filtered_adj)).t(),\n",
    "                values=torch.tensor(filtered_adj)[torch.nonzero(torch.tensor(filtered_adj), as_tuple=True)],\n",
    "                size=filtered_adj.shape\n",
    "            ).to(data.A.device)\n",
    "        else:\n",
    "            train_data.A = torch.tensor(filtered_adj, device=data.A.device, dtype=data.A.dtype)\n",
    "    \n",
    "    # 8. Filter tabular data (numpy arrays)\n",
    "    if hasattr(data, 'arr_X') and data.arr_X is not None:\n",
    "        train_data.arr_X = data.arr_X[mask_np]\n",
    "    \n",
    "    if hasattr(data, 'arr_YF') and data.arr_YF is not None:\n",
    "        train_data.arr_YF = data.arr_YF[mask_np]\n",
    "    \n",
    "    if hasattr(data, 'arr_Y1') and data.arr_Y1 is not None:\n",
    "        train_data.arr_Y1 = data.arr_Y1[mask_np]\n",
    "    \n",
    "    if hasattr(data, 'arr_Y0') and data.arr_Y0 is not None:\n",
    "        train_data.arr_Y0 = data.arr_Y0[mask_np]\n",
    "    \n",
    "    # Filter multi-dimensional adjacency matrix\n",
    "    if hasattr(data, 'arr_Adj') and data.arr_Adj is not None:\n",
    "        node_indices = np.where(mask_np)[0]\n",
    "        if len(data.arr_Adj.shape) == 2:  # Single adjacency matrix\n",
    "            train_data.arr_Adj = data.arr_Adj[np.ix_(node_indices, node_indices)]\n",
    "        elif len(data.arr_Adj.shape) == 3:  # Multi-relational adjacency matrices\n",
    "            train_data.arr_Adj = data.arr_Adj[:, np.ix_(node_indices, node_indices)]\n",
    "    \n",
    "    # 9. Filter dataframes\n",
    "    if hasattr(data, 'df_full') and data.df_full is not None:\n",
    "        train_data.df_full = data.df_full[mask_np].reset_index(drop=True)\n",
    "    \n",
    "    if hasattr(data, 'df_miss') and data.df_miss is not None:\n",
    "        train_data.df_miss = data.df_miss[mask_np].reset_index(drop=True)\n",
    "    \n",
    "    if hasattr(data, 'df_imputed') and data.df_imputed is not None:\n",
    "        train_data.df_imputed = data.df_imputed[mask_np].reset_index(drop=True)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "# Example usage functions\n",
    "def create_train_val_data(data: Data) -> Data:\n",
    "    \"\"\"\n",
    "    Create dataset with only train and validation nodes.\n",
    "    \"\"\"\n",
    "    mask = data.train_mask | data.val_mask\n",
    "    return filter_dataset(data, mask)\n",
    "\n",
    "\n",
    "def create_train_data_only(data: Data) -> Data:\n",
    "    \"\"\"\n",
    "    Create dataset with only training nodes.\n",
    "    \"\"\"\n",
    "    return filter_dataset(data, data.train_mask)\n",
    "\n",
    "\n",
    "def create_test_data_only(data: Data) -> Data:\n",
    "    \"\"\"\n",
    "    Create dataset with only test nodes.\n",
    "    \"\"\"\n",
    "    return filter_dataset(data, data.test_mask)\n",
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "full_data_path = '/Users/jason/Documents/Coding Projects/2025_Claude/NetDeconf_main_hao/datasets/exps/BlogCatalog/p=0.0_k=9_seed=194.pt'\n",
    "# full_data_path = 'datasets/exps/Syn/p=0.0_k=0_seed=919.pt'\n",
    "\n",
    "data = torch.load(full_data_path, weights_only=False)\n",
    "create_train_data_only(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1c3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True, False, False, False, False,  True,  True,\n",
      "         True,  True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example inputs\n",
    "mask = torch.tensor([True, False, True], dtype=torch.bool)  # shape (N,)\n",
    "n_attrs = 4\n",
    "\n",
    "# Expand mask\n",
    "attrs_mask = mask.repeat_interleave(n_attrs)\n",
    "\n",
    "print(attrs_mask)\n",
    "# Output: tensor([ True,  True,  True,  True, False, False, False, False,  True,  True,  True,  True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def filter_and_remap_edges_v1(rel_edge_index, non_null_indicator):\n",
    "    \"\"\"\n",
    "    Filter edges and remap node indices to consecutive integers.\n",
    "    \n",
    "    Args:\n",
    "        rel_edge_index: torch.Tensor of shape [2, E] where each column represents an edge\n",
    "        non_null_indicator: torch.Tensor of shape [N,] with boolean values indicating valid nodes\n",
    "    \n",
    "    Returns:\n",
    "        new_rel_edge_index: torch.Tensor of shape [2, E'] with filtered and remapped edges\n",
    "        node_mapping: torch.Tensor of shape [N,] mapping old indices to new indices (-1 for invalid nodes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create mapping from old indices to new consecutive indices\n",
    "    # Only nodes with non_null_indicator[i] == True get new indices\n",
    "    valid_nodes = torch.where(non_null_indicator)[0]  # indices where non_null_indicator is True\n",
    "    \n",
    "    # Create mapping: old_index -> new_index (-1 for invalid nodes)\n",
    "    node_mapping = torch.full((non_null_indicator.size(0),), -1, dtype=torch.long)\n",
    "    node_mapping[valid_nodes] = torch.arange(len(valid_nodes))\n",
    "    \n",
    "    # Step 2: Find which edges to keep\n",
    "    # An edge is kept if both source and target nodes are valid (non_null_indicator == True)\n",
    "    source_nodes = rel_edge_index[0]  # shape: [E]\n",
    "    target_nodes = rel_edge_index[1]  # shape: [E]\n",
    "    \n",
    "    # Keep edge only if both source and target are valid\n",
    "    keep_edge = non_null_indicator[source_nodes] & non_null_indicator[target_nodes]\n",
    "    \n",
    "    # Step 3: Filter edges and apply mapping\n",
    "    filtered_source = node_mapping[source_nodes[keep_edge]]\n",
    "    filtered_target = node_mapping[target_nodes[keep_edge]]\n",
    "    \n",
    "    new_rel_edge_index = torch.stack([filtered_source, filtered_target])\n",
    "    \n",
    "    return new_rel_edge_index, node_mapping\n",
    "\n",
    "\n",
    "def filter_and_remap_edges(rel_edge_index, non_null_indicator):\n",
    "    \"\"\"\n",
    "    Most optimized version - combines operations and minimizes memory allocations.\n",
    "    \"\"\"\n",
    "    # Get valid node indices using cumsum (more efficient than torch.where for dense cases)\n",
    "    valid_mask = non_null_indicator\n",
    "    \n",
    "    # Create compact mapping using cumsum\n",
    "    mapping = torch.cumsum(valid_mask, dim=0) - 1\n",
    "    mapping[~valid_mask] = -1  # Mark invalid nodes\n",
    "    \n",
    "    # Filter and remap in one step\n",
    "    edge_mask = valid_mask[rel_edge_index[0]] & valid_mask[rel_edge_index[1]]\n",
    "    filtered_edges = rel_edge_index[:, edge_mask]\n",
    "    \n",
    "    # Apply mapping directly\n",
    "    new_rel_edge_index = mapping[filtered_edges]\n",
    "    \n",
    "    return new_rel_edge_index, mapping\n",
    "# Example usage and test\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: 5 nodes, some edges\n",
    "    rel_edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3, 1, 4, 5, 2],  # source nodes\n",
    "        [1, 2, 3, 4, 4, 0, 1, 5]   # target nodes\n",
    "    ])\n",
    "    \n",
    "    # Mark nodes 1 and 3 as invalid (False), others as valid (True)\n",
    "    non_null_indicator = torch.tensor([False, True, True, False, True, True])\n",
    "    \n",
    "    print(\"Original rel_edge_index:\")\n",
    "    print(rel_edge_index)\n",
    "    print(\"\\nNon-null indicator:\", non_null_indicator)\n",
    "    print(\"Valid nodes:\", torch.where(non_null_indicator)[0].tolist())\n",
    "    \n",
    "    new_rel_edge_index, node_mapping = filter_and_remap_edges(rel_edge_index, non_null_indicator)\n",
    "    \n",
    "    print(\"\\nNode mapping (old -> new):\")\n",
    "    for i, new_idx in enumerate(node_mapping):\n",
    "        if new_idx != -1:\n",
    "            print(f\"  Node {i} -> Node {new_idx}\")\n",
    "        else:\n",
    "            print(f\"  Node {i} -> REMOVED\")\n",
    "    \n",
    "    print(f\"\\nFiltered and remapped rel_edge_index:\")\n",
    "    print(new_rel_edge_index)\n",
    "    print(f\"Number of edges: {rel_edge_index.shape[1]} -> {new_rel_edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19839784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def cartesian_embeddings_broadcast(node_emb, attr_emb):\n",
    "    \"\"\"The code to test.\"\"\"\n",
    "    N, M, K = node_emb.size(0), attr_emb.size(0), node_emb.size(1)\n",
    "    \n",
    "    # Use expand for memory efficiency (creates views, not copies)\n",
    "    node_expanded = node_emb.unsqueeze(1).expand(N, M, K)\n",
    "    attr_expanded = attr_emb.unsqueeze(0).expand(N, M, K)\n",
    "    \n",
    "    # Concatenate and reshape\n",
    "    combined = torch.cat([node_expanded, attr_expanded], dim=2)\n",
    "    return combined.view(N * M, 2 * K)\n",
    "\n",
    "# Test 1: Basic functionality and shape verification\n",
    "def test_basic_functionality():\n",
    "    print(\"=== Test 1: Basic Functionality ===\")\n",
    "    N, M, K = 3, 2, 4\n",
    "    \n",
    "    node_emb = torch.randn(N, K)\n",
    "    attr_emb = torch.randn(M, K)\n",
    "    \n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    \n",
    "    # Check shape\n",
    "    expected_shape = (N * M, 2 * K)\n",
    "    assert result.shape == expected_shape, f\"Expected {expected_shape}, got {result.shape}\"\n",
    "    \n",
    "    print(f\"✓ Shape correct: {result.shape}\")\n",
    "    print(f\"✓ Input shapes: node_emb={node_emb.shape}, attr_emb={attr_emb.shape}\")\n",
    "\n",
    "# Test 2: Value correctness - verify the indexing pattern\n",
    "def test_value_correctness():\n",
    "    print(\"\\n=== Test 2: Value Correctness ===\")\n",
    "    N, M, K = 3, 2, 3\n",
    "    \n",
    "    # Create simple test data with known values\n",
    "    node_emb = torch.arange(N * K, dtype=torch.float32).reshape(N, K)\n",
    "    attr_emb = torch.arange(M * K, dtype=torch.float32).reshape(M, K) + 100\n",
    "    \n",
    "    print(\"Node embeddings:\")\n",
    "    print(node_emb)\n",
    "    print(\"Attr embeddings:\")\n",
    "    print(attr_emb)\n",
    "    \n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    \n",
    "    print(\"\\nResult:\")\n",
    "    print(result)\n",
    "    \n",
    "    # Verify specific entries\n",
    "    # result[0] should be [node_emb[0], attr_emb[0]]\n",
    "    expected_0 = torch.cat([node_emb[0], attr_emb[0]])\n",
    "    assert torch.allclose(result[0], expected_0), f\"Index 0 mismatch\"\n",
    "    \n",
    "    # result[1] should be [node_emb[0], attr_emb[1]]\n",
    "    expected_1 = torch.cat([node_emb[0], attr_emb[1]])\n",
    "    assert torch.allclose(result[1], expected_1), f\"Index 1 mismatch\"\n",
    "    \n",
    "    # result[2] should be [node_emb[1], attr_emb[0]]\n",
    "    expected_2 = torch.cat([node_emb[1], attr_emb[0]])\n",
    "    assert torch.allclose(result[2], expected_2), f\"Index 2 mismatch\"\n",
    "    \n",
    "    print(\"✓ All indexing patterns correct\")\n",
    "\n",
    "# Test 3: Comprehensive indexing verification\n",
    "def test_comprehensive_indexing():\n",
    "    print(\"\\n=== Test 3: Comprehensive Indexing ===\")\n",
    "    N, M, K = 4, 3, 2\n",
    "    \n",
    "    # Use sequential values for easy verification\n",
    "    node_emb = torch.arange(N * K, dtype=torch.float32).reshape(N, K)\n",
    "    attr_emb = torch.arange(M * K, dtype=torch.float32).reshape(M, K) + 1000\n",
    "    \n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    \n",
    "    # Verify every entry follows the pattern: result[n*M+m] = [node_emb[n], attr_emb[m]]\n",
    "    for n in range(N):\n",
    "        for m in range(M):\n",
    "            idx = n * M + m\n",
    "            expected = torch.cat([node_emb[n], attr_emb[m]])\n",
    "            actual = result[idx]\n",
    "            \n",
    "            assert torch.allclose(actual, expected), \\\n",
    "                f\"Mismatch at n={n}, m={m}, idx={idx}: expected {expected}, got {actual}\"\n",
    "    \n",
    "    print(f\"✓ All {N*M} combinations verified correctly\")\n",
    "\n",
    "# Test 4: Edge cases\n",
    "def test_edge_cases():\n",
    "    print(\"\\n=== Test 4: Edge Cases ===\")\n",
    "    \n",
    "    # Single node, multiple attributes\n",
    "    node_emb = torch.randn(1, 5)\n",
    "    attr_emb = torch.randn(10, 5)\n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    assert result.shape == (10, 10)\n",
    "    print(\"✓ Single node case passed\")\n",
    "    \n",
    "    # Multiple nodes, single attribute\n",
    "    node_emb = torch.randn(8, 3)\n",
    "    attr_emb = torch.randn(1, 3)\n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    assert result.shape == (8, 6)\n",
    "    print(\"✓ Single attribute case passed\")\n",
    "    \n",
    "    # Single node, single attribute\n",
    "    node_emb = torch.randn(1, 4)\n",
    "    attr_emb = torch.randn(1, 4)\n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    assert result.shape == (1, 8)\n",
    "    print(\"✓ Single-single case passed\")\n",
    "\n",
    "# Test 5: Large tensor performance\n",
    "def test_large_tensors():\n",
    "    print(\"\\n=== Test 5: Large Tensor Performance ===\")\n",
    "    import time\n",
    "    \n",
    "    N, M, K = 1000, 500, 128\n",
    "    node_emb = torch.randn(N, K)\n",
    "    attr_emb = torch.randn(M, K)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    expected_shape = (N * M, 2 * K)\n",
    "    assert result.shape == expected_shape\n",
    "    \n",
    "    print(f\"✓ Large tensor test passed: {result.shape}\")\n",
    "    print(f\"✓ Time taken: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Test 6: Gradient preservation\n",
    "def test_gradient_preservation():\n",
    "    print(\"\\n=== Test 6: Gradient Preservation ===\")\n",
    "    \n",
    "    N, M, K = 3, 2, 4\n",
    "    node_emb = torch.randn(N, K, requires_grad=True)\n",
    "    attr_emb = torch.randn(M, K, requires_grad=True)\n",
    "    \n",
    "    result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    \n",
    "    # Compute a simple loss\n",
    "    loss = result.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    assert node_emb.grad is not None, \"Node embedding gradients not computed\"\n",
    "    assert attr_emb.grad is not None, \"Attribute embedding gradients not computed\"\n",
    "    assert node_emb.grad.shape == node_emb.shape, \"Node gradient shape mismatch\"\n",
    "    assert attr_emb.grad.shape == attr_emb.shape, \"Attribute gradient shape mismatch\"\n",
    "    \n",
    "    print(\"✓ Gradients computed correctly\")\n",
    "    print(f\"✓ Node grad shape: {node_emb.grad.shape}\")\n",
    "    print(f\"✓ Attr grad shape: {attr_emb.grad.shape}\")\n",
    "\n",
    "# Test 7: Different data types and devices\n",
    "def test_dtypes_and_devices():\n",
    "    print(\"\\n=== Test 7: Data Types and Devices ===\")\n",
    "    \n",
    "    N, M, K = 2, 3, 4\n",
    "    \n",
    "    # Test different dtypes\n",
    "    for dtype in [torch.float32, torch.float64, torch.float16]:\n",
    "        node_emb = torch.randn(N, K, dtype=dtype)\n",
    "        attr_emb = torch.randn(M, K, dtype=dtype)\n",
    "        result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "        \n",
    "        assert result.dtype == dtype, f\"Dtype not preserved: expected {dtype}, got {result.dtype}\"\n",
    "        assert result.shape == (N * M, 2 * K)\n",
    "    \n",
    "    print(\"✓ All data types work correctly\")\n",
    "    \n",
    "    # Test GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        node_emb = torch.randn(N, K, device=device)\n",
    "        attr_emb = torch.randn(M, K, device=device)\n",
    "        result = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "        \n",
    "        assert result.device == device, \"Device not preserved\"\n",
    "        assert result.shape == (N * M, 2 * K)\n",
    "        print(\"✓ GPU test passed\")\n",
    "    else:\n",
    "        print(\"✓ GPU not available, skipping GPU test\")\n",
    "\n",
    "# Test 8: Memory efficiency verification\n",
    "def test_memory_efficiency():\n",
    "    print(\"\\n=== Test 8: Memory Efficiency ===\")\n",
    "    \n",
    "    N, M, K = 100, 50, 64\n",
    "    node_emb = torch.randn(N, K)\n",
    "    attr_emb = torch.randn(M, K)\n",
    "    \n",
    "    # Get initial memory\n",
    "    initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    \n",
    "    # Create intermediate tensors to verify they use views\n",
    "    node_expanded = node_emb.unsqueeze(1).expand(N, M, K)\n",
    "    attr_expanded = attr_emb.unsqueeze(0).expand(N, M, K)\n",
    "    \n",
    "    # Verify that expand creates views (shares storage)\n",
    "    assert node_expanded.storage().data_ptr() == node_emb.storage().data_ptr(), \\\n",
    "        \"expand() should create views, not copies\"\n",
    "    \n",
    "    print(\"✓ Memory efficiency verified - expand() creates views\")\n",
    "\n",
    "# Test 9: Comparison with naive implementation\n",
    "def test_comparison_with_naive():\n",
    "    print(\"\\n=== Test 9: Comparison with Naive Implementation ===\")\n",
    "    \n",
    "    def naive_cartesian(node_emb, attr_emb):\n",
    "        \"\"\"Naive nested loop implementation for comparison.\"\"\"\n",
    "        N, M, K = node_emb.size(0), attr_emb.size(0), node_emb.size(1)\n",
    "        result = torch.zeros(N * M, 2 * K, dtype=node_emb.dtype, device=node_emb.device)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for m in range(M):\n",
    "                idx = n * M + m\n",
    "                result[idx] = torch.cat([node_emb[n], attr_emb[m]])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    N, M, K = 5, 4, 6\n",
    "    node_emb = torch.randn(N, K)\n",
    "    attr_emb = torch.randn(M, K)\n",
    "    \n",
    "    result_broadcast = cartesian_embeddings_broadcast(node_emb, attr_emb)\n",
    "    result_naive = naive_cartesian(node_emb, attr_emb)\n",
    "    \n",
    "    assert torch.allclose(result_broadcast, result_naive, atol=1e-6), \\\n",
    "        \"Results don't match between broadcast and naive implementation\"\n",
    "    \n",
    "    print(\"✓ Results match naive implementation perfectly\")\n",
    "\n",
    "# Run all tests\n",
    "if __name__ == \"__main__\":\n",
    "    tests = [\n",
    "        test_basic_functionality,\n",
    "        test_value_correctness,\n",
    "        test_comprehensive_indexing,\n",
    "        test_edge_cases,\n",
    "        test_large_tensors,\n",
    "        test_gradient_preservation,\n",
    "        test_dtypes_and_devices,\n",
    "        test_memory_efficiency,\n",
    "        test_comparison_with_naive\n",
    "    ]\n",
    "    \n",
    "    print(\"Running comprehensive tests for cartesian embeddings...\\n\")\n",
    "    \n",
    "    for test in tests:\n",
    "        try:\n",
    "            test()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {test.__name__} FAILED: {e}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"\\n🎉 All tests passed! The implementation is correct.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
